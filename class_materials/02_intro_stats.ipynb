{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 2: Linear Algebra and Stats in Python (continued)\n",
    "\n",
    "### Outline of this notebook\n",
    "* **getting some data**\n",
    "* **mean, median, mode**\n",
    "* **variance and standard deviation**\n",
    "* **covariance and correlation**\n",
    "* **histograms**\n",
    "* **scatter plots**\n",
    "* **random variables and distributions**\n",
    "* **the normal distribution - useful properties**\n",
    "* **the exponential distribution - useful properties**\n",
    "* **important properties of empirical distributions - skew and kurtosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at any collection of data, you want to be able to look at just a few numbers that provide a useful indication of how the whole collection behaves.\n",
    "\n",
    "Statistics is the study of those small sets of numbers that provide a useful indication of how any collection of numbers (arbitrarily large) behavies.\n",
    "\n",
    "We call these collections of numbers **distributions** and the smaller set of numbers that describe them as the **properties of distributions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, division\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\": (10, 6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets get some data that we can actually work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get iris dataset from online\n",
    "r = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data') \n",
    "\n",
    "iris_data = [row.split(\",\") for row in r.iter_lines()]\n",
    "\n",
    "print(\"First five entries in iris_data:\")\n",
    "for row in iris_data[0:5]:\n",
    "    print(row)\n",
    "print(\"Last five entries in iris_data:\")\n",
    "for row in iris_data[-5:]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to have to clean it up a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove last line as it is empty\n",
    "iris_data_cleaned = iris_data[:-1]\n",
    "\n",
    "#Put last element in each entry into separate list\n",
    "iris_names = [row[-1] for row in iris_data_cleaned]\n",
    "print(\"First five entries in iris_names:\")\n",
    "for row in iris_names[0:5]:\n",
    "    print(row)\n",
    "\n",
    "#Put all elements but last in each entry into values list\n",
    "iris_data_values = [row[:-1] for row in iris_data_cleaned]\n",
    "print(\"First five entries in iris_data_cleaned:\")\n",
    "for row in iris_data_values[0:5]:\n",
    "    print(row)\n",
    "\n",
    "#Convert values into floats\n",
    "iris_data_values_float = [[float(value) for value in row] for row in iris_data_values]\n",
    "print(\"First five entries in iris_data_values_float:\")\n",
    "for row in iris_data_values_float[0:5]:\n",
    "    print(row)\n",
    "\n",
    "#Convert list of list of floats into matrix\n",
    "iris_data_final = np.array(iris_data_values_float)\n",
    "print(\"First five entries in iris_data_final:\")\n",
    "for row in iris_data_final[0:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iris_data_final` contains the properly formatted numeric data, so we will be using that to illustrate all of the statistics going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean, median, mode\n",
    "\n",
    "The **mean** is the arithmetic average of a group of values, found by dividing the total of all values by the number of values.\n",
    "\n",
    "The **median** is the middle value in a group of values, found by ordering the values from smallest to largest and locating the one that occurs in the middle. If the size of the group is even, it is found by averaging the middle two values.\n",
    "\n",
    "The **mode** is the value that occurs most often in a group of values, and is found by counting the frequency of every distinct value in the group and outputting the one that occurs most frequently.\n",
    "\n",
    "Let's compute each of these properties in turn on the columns in the `iris_data_final` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the data (rows,columns):\",iris_data_final.shape)\n",
    "print(\"Mean of each column:\",iris_data_final.mean(axis=0))\n",
    "#must pass in an axis argument, otherwise will compute mean of entire matrix\n",
    "print(\"Median of each column:\",np.median(iris_data_final,axis=0))\n",
    "print(\"Mode of each column:\",stats.mode(iris_data_final)) \n",
    "#implied axis=0 for this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance and standard deviation\n",
    "\n",
    "It is useful to get a sense of how the values in a dataset are spread about the average or the central value in that dataset. **Variance** and **standard deviation** are values that quantify that spread.\n",
    "\n",
    "The **variance** of a set of values, **$\\sigma^2$**, is the average of the square of the difference of the values in the dataset from the dataset's mean:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\\n",
    "\\end{align}\n",
    "\n",
    "We square the differences to preserve the magnitude of the difference only (otherwise we would be adding and subtracting values, and lose the average magnitude). So, the variance is not on the same scale as the values themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Variance of each column:\\n\",iris_data_final.var(axis=0))\n",
    "#must pass in an axis argument, otherwise will compute mean of entire matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **standard deviation**, $\\sigma$, is the square root of the variance:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\\n",
    "\\end{align}\n",
    "\n",
    "We use the **standard deviation** much more regularly than the **variance** because it is on the same scale as the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Standard deviation of each column:\\n\",iris_data_final.std(axis=0))\n",
    "#must pass in an axis argument, otherwise will compute mean of entire matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariance and correlation\n",
    "\n",
    "So far, we've been looking at statistical properties of individual collections of data (in our case, columns of the `iris_data_final` matrix) by themselves. However, it is also useful to quantify how two collections of data vary with each other. **Covariance** and **correlation** measure this exact quantity.\n",
    "\n",
    "**Covariance**, like the variance, is a measure of spread, however it also measures how closely two datasets track each other. That is, **covariance** describes both how far the variables are spread out, and the nature of their relationship, whether they tend to increase and decrease together, or whether they do not track each other at all. Like **variance**,  the **covariance** is a squared quantity (again, to simply measure the magnitude of the spread and to avoid subtracting numbers from each other) so it is not on the same scale as the mean (and the **covariance** of different pairs of variables can have completely different scales):\n",
    "\n",
    "\\begin{align}\n",
    "Cov(x,y) = \\frac{\\sum_{i=1}^{n}(x_i - \\mu_x)(y_i - \\mu_y)} {n}\\\n",
    "\\end{align}\n",
    "\n",
    "When we look at the covariance of a dataset, we compute what is called a **covariance matrix**.\n",
    "\n",
    "This matrix is symmetric and tells us what all the paired covariance values are for every pair of rows (or columns) in our original matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#have to take the transpose of iris_data_final, because np.cov computes row-based covariances\n",
    "#so if we take the transpose, our columns become our rows!\n",
    "iris_data_final_column_cov = np.cov(iris_data_final.T)\n",
    "print(\"Original iris_data_final matrix shape:\",iris_data_final.shape)\n",
    "print(\"Columnar covariance matrix:\\n\",iris_data_final_column_cov,\n",
    "      iris_data_final_column_cov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation** is sort of like **standard deviation** generalized to pairs of datasets.\n",
    "\n",
    "Really, the **correlation** is a **covariance** scaled by each dataset's **standard deviation**, so that it can only take on values from -1 to +1. This allows us to compare two pairs of variables and quickly tell if one pair of datasets is more related than an other (since both values are scaled to the same range!):\n",
    "\n",
    "\\begin{align}\n",
    "r(x,y) = \\frac{cov(x,y)} {\\sigma_x\\sigma_y}\\\n",
    "\\end{align}\n",
    "\n",
    "Or this:\n",
    "\n",
    "\\begin{align}\n",
    "r(x,y) = \\frac{\\frac{\\sum_{i=1}^{n}(x_i-\\mu_x)(y_i-\\mu_y)} {n}} {\\sigma_x\\sigma_y}\\\n",
    "\\end{align}\n",
    "\n",
    "A positive correlation indicates the sets of values change together (when one increases or decreases the other does the same). A negative correlation indicates that the variables change in opposite directions (when one increases the other decreases).\n",
    "\n",
    "Just like the **covariance**, when we compute **correlations** for some dataset, we really compute a **correlation matrix** over all of the distinct sets of values in the dataset. The **correlation matrix** is symmetric, just like the **covariance matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#again, we have to take the transpose because otherwise we will get row-wise correlations.\n",
    "iris_data_final_column_corr = np.corrcoef(iris_data_final.T)\n",
    "print(\"Original iris_data_final matrix shape:\",iris_data_final.shape)\n",
    "print(\"Columnar covariance matrix:\\n\",iris_data_final_column_corr,\n",
    "      iris_data_final_column_corr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the correlation matrix visually too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(iris_data_final_column_corr)\n",
    "print(\"The correlation matrix for the iris dataset:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!\n",
    "* Compute the row-based correlation matrix for `iris_data_final`.\n",
    "* Visualize the correlation matrix as a heatmap. Notice anything?\n",
    "* I've loaded in vertebral_data into a `NumPy` matrix called `vertebral_values_final` for you below:\n",
    "    * Compute the mean of each column. \n",
    "    * Compute the median of each column.\n",
    "    * Compute the mode of each column.\n",
    "    * What would it mean if the mean and the median for a given column are very far apart?\n",
    "    * What is useful about knowing the mode?\n",
    "    * Compute the variance and standard deviation of each column.\n",
    "    * Generate the columnar covariance and correlation matrices for this matrix.\n",
    "    * Do any columns appear to change together (based on their covariances/correlations)?\n",
    "    * What conclusions can we draw from these column-based statistics?\n",
    "    * If we had computed all of the row-based statistics here, what would their interpretation be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/vertebral_column_2_categories.dat', 'r') as f:\n",
    "    vertebral_data = [row for row in csv.reader(f)]\n",
    "\n",
    "vertebral_data_parsed = [str.split(row[0],' ') for row in vertebral_data]\n",
    "vertebral_names = [row[-1] for row in vertebral_data_parsed]\n",
    "vertebral_values = [row[:-1] for row in vertebral_data_parsed]\n",
    "vertebral_values_float = [[float(value) for value in row] for row in vertebral_values]\n",
    "vertebral_values_final = np.array(vertebral_values_float)\n",
    "print(\"vertebral_values_final = \\n\", vertebral_values_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is where you write the code for your exercise.\n",
    "#Make sure you execute the cell just above this one so that vertebral_values_final is populated.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histograms\n",
    "\n",
    "A regular histogram is a very powerful, simple way to get a sense of the kinds of values a single set of data is comprised of. \n",
    "\n",
    "Let's take a look at the histogram of values for the first column in `iris_data_final`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(iris_data_final[:,0],kde=False,bins=15)\n",
    "print(\"First column of the iris dataset:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **x-axis shows us the actual range of values that the column takes on**. It looks like the first column ranges from just over 4 to just under 8.\n",
    "\n",
    "The **y-axis is a frequency count of the number of rows in the column that fall into each vertical bin**. So, the first bin (furthest to the left), which ranges from ~4.25 to ~4.75 contains 11 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make **cumulative history histograms**, which give us a graphical way to see what fraction of the datapoints are below or above a certain value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(iris_data_final[:,0], hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "print(\"Cumulative frequency plot of 1st column:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter plot\n",
    "\n",
    "When you want to visualize how two columns vary together, you can use a **scatter plot**. \n",
    "\n",
    "A scatter plot simply plots the value for each column per-row for the two columns you've chosen.\n",
    "\n",
    "Every dot in the scatter plot is an individual row. \n",
    "\n",
    "Let's generate the scatter plot for the first two columns in the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.plt.scatter(iris_data_final[:,0], iris_data_final[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine both a **scatter plot** and a **histogram** for a pair of columns very effectively using a `jointplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(iris_data_final[:,1], iris_data_final[:,0],stat_func=None)\n",
    "print(\"Scatter plot with histograms for the first two columns in the iris dataset:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Time!\n",
    "* Using the `vertebral_values_final` dataset:\n",
    "    * Compute the column-wise correlation matrix\n",
    "    * Visualize the scatter plot for two columns that are positively correlated\n",
    "    * Visualize the scatter plot for two columns that are negatively correlated (anti-correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random variables and distributions \n",
    "\n",
    "A **random variable** is a variable whose value is subject to variations due to chance. A random variable can take on a set of possible different values (similarly to other mathematical variables), each with an associated probability. [random variable wiki](https://en.wikipedia.org/wiki/Random_variable)\n",
    "\n",
    "There are two kinds of random variables: **discrete** and **continuous**\n",
    "\n",
    "You know about **discrete** random variables, because you've flipped a coin (at some point in your life).\n",
    "\n",
    "For an unbiased coin, the random variable can take on the values **heads** or **tails**, each with probability $p= 0.5$ but you don't know when it will take on the value. Let's say we flipped a coin ten times, how many heads will we get?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "prob_heads = 0.5\n",
    "num_heads = np.random.binomial(n_trials, prob_heads)\n",
    "print(\"Num heads:\",num_heads)\n",
    "print(\"Fraction heads:\",float(num_heads)/n_trials) #this should be different for most people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of us should have different numbers! That's because you can only talk about the probabilities, not the outcomes (you only know the outcome of an experiment after it happens).\n",
    "\n",
    "What about **continuous** random variables?\n",
    "\n",
    "A **continuous random variable** is a random variable where the data can take infinitely many values.\n",
    "\n",
    "Think about how long it takes you to make your bed every morning. The experiment \"how long it take me to make my bed\" is a **continuous random variable** because it can take one of an infinitely many distinct times (30 seconds, 2.54 minutes, 0 minutes because he doesnt wake up on time and runs to work, etc.).\n",
    "\n",
    "Let's look at the amount of time it took me to make his bed over the last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bed_making_minutes = np.random.exponential(size=7) \n",
    "## This is generating 7 experiments from a specific probability distribution\n",
    "print(bed_making_minutes)\n",
    "# Everyone's values should be different, compare with your neighbor to make sure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Time!!!\n",
    "\n",
    "* Generate 100 coin flips, what fraction of them come up heads?\n",
    "* Generate 1,000 coin flips, what fraction of them come up heads?\n",
    "* Generate 100,000 coin flips, what fraction of them come up heads? Notice a pattern?\n",
    "* Generate 100 bed making experiments, whats the average bed making time?\n",
    "* Generate 1,000 bed making experiments, whats the average bed making time?\n",
    "* Generate 100,000 bed making experiments, whats the average bed making time? Notice a pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **probability distribution** is a table or an equation that links each outcome of a statistical experiment with its probability of occurrence. [probability distribution wiki](https://en.wikipedia.org/wiki/Probability_distribution)\n",
    "\n",
    "That is, it is a distribution of probabilities over the values that a **random variable** can take on.\n",
    "\n",
    "There are lots of different kinds of distributions that statisticians have identified and studied. We will talk about just a few of them.\n",
    "\n",
    "**All of data science boils down to trying to find/capture differences between probability distributions across classes of objects, and combining those differences across many distributions into aggregate predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the binomial distribution\n",
    "\n",
    "The **binomial distribution** is a distribution over discrete values. It has two parameters, `n` and `p`, and is the discrete probability distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields success (yes) with probability `p`.\n",
    "\n",
    "A success/failure experiment is also called a Bernoulli experiment or Bernoulli trial; when n = 1 (a single coin flip), the binomial distribution is a Bernoulli distribution  [binomial distribution wiki](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "\n",
    "So, heres the approximate probability distribution of the number of heads in 10 coin flips.\n",
    "\n",
    "We are going to generate the distribution by simulating 10,000 experiments where we flip a coin 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trials_binomial=1000\n",
    "binomial_data = np.random.binomial(n_trials, prob_heads,n_trials_binomial)\n",
    "sns.plt.hist(binomial_data,normed=True,alpha=0.5,bins=10)\n",
    "print(\"A binomial distribution for the number of heads in 10 coin flips:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the normal distribution\n",
    "\n",
    "The normal distribution is the most important distribution in all of statistics. It is a continuous probability distribution, unlike the binomial, which is discrete.\n",
    "\n",
    "It's super important because as datasets become larger and larger and larger, they tend to look more and more like the normal distribution.\n",
    "\n",
    "The normal distribution is fully specified by 2 parameters, $\\mu$ (its mean) and $\\sigma$ (its standard deviation).\n",
    "\n",
    "Nothing in the world actually perfectly mirrors a normal distribution, because its an idealized, mathematical object. It is simply a theoretical approximation of the kinds of things we tend to see in the world. However, there are many datasets that behave very similarly to a normal distribution. [normal distribution wiki](https://en.wikipedia.org/wiki/Normal_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 1\n",
    "n_samples = 100000\n",
    "normal_data = np.random.normal(mu, sigma, n_samples)\n",
    "sns.distplot(normal_data)\n",
    "print(\"100000 values sampled from the normal distribution with mean=0 standard deviation=1:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "print(\"CDF of normal distribution:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altering the $\\mu$ of the distribution alters its location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_mu = -3\n",
    "positive_mu = 3\n",
    "negative_mu_normal_data = np.random.normal(negative_mu, sigma, n_samples)\n",
    "positive_mu_normal_data = np.random.normal(positive_mu, sigma, n_samples)\n",
    "sns.distplot(negative_mu_normal_data)\n",
    "sns.distplot(positive_mu_normal_data)\n",
    "sns.distplot(normal_data)\n",
    "\n",
    "print(\"100000 values sampled from three normal distributions with \" +\n",
    "      \"different mean=-3,0,5 and same standard deviation= 1:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(negative_mu_normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(positive_mu_normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "print(\"CDF of three normal distributions with different mean=-3,0,5 \" +\n",
    "      \"and same standard deviation= 1:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altering the $\\sigma$ of the distribution alters its width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "larger_sigma = 3\n",
    "largest_sigma = 5\n",
    "#Generate 2 collections of 100,000 samples from each distribution. \n",
    "larger_sigma_normal_data = np.random.normal(mu, larger_sigma, n_samples)\n",
    "largest_sigma_normal_data = np.random.normal(mu, largest_sigma, n_samples)\n",
    "#plot all 3 distributions on the same plot\n",
    "sns.distplot(largest_sigma_normal_data)\n",
    "sns.distplot(larger_sigma_normal_data)\n",
    "sns.distplot(normal_data)\n",
    "print(\"100,000 values sampled from three normal distributions with \"\n",
    "      \"the same mean = 0 but different standard deviations = 1,3,5:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(largest_sigma_normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(larger_sigma_normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(normal_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.plt.xlim((-10,10))\n",
    "print(\"CDF for three normal distributions with the same mean = 0 \" \n",
    "      \"but different standard deviations = 1,3,5:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the exponential distribution\n",
    "\n",
    "The **exponential distribution** is another very common distribution (and the one we used for my bed-making example). In stats-speak, it is the probability distribution that describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. [exponential distribution wiki]()\n",
    "\n",
    "An example exponential distribution would describe the time between airplanes landing and taking off at LaGuardia airport during hours that the airport is operating.\n",
    "\n",
    "The exponential distribution is interesing because it has the property known as being **memoryless**. This means that knowing the history of the distribution has no effect on being able to predict what will happen next.\n",
    "\n",
    "If the average time between airplane landing/takeoff is 125s, but for the past 5 times it was 30s, 42s, 50s, 10s, and 40s, your best guess about the next successive take/off landing time difference is still 125s (there are no hot streaks in airplane landings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = np.random.exponential(size=n_samples)\n",
    "sns.plt.hist(e,bins=100,normed=True)\n",
    "print(\"The standard exponential distribution:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(e, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.plt.xlim((0,11))\n",
    "print(\"CDF of exponential distribution:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "larger_beta = 3\n",
    "largest_beta = 5\n",
    "e2 = np.random.exponential(scale = larger_beta, size=n_samples)\n",
    "e3 = np.random.exponential(scale = largest_beta, size=n_samples)\n",
    "sns.plt.hist(e3,bins=100,alpha=0.5,hold=True)\n",
    "sns.plt.hist(e2,bins=100,alpha=0.5,hold=True)\n",
    "sns.plt.hist(e,bins=100,hold=True)\n",
    "print(\"100000 values sampled from three exponential distributions with beta=1,3,5:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(e, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(e2, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.distplot(e3, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})\n",
    "sns.plt.xlim((0,20))\n",
    "print(\"CDF of three exponential distributions with beta=1,3,5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FYI:** $\\beta$ is the same as $\\frac {1} {\\lambda}$ for the other common parameterization of the exponential distribution. \n",
    "\n",
    "I like to use $\\beta$ instead of $\\lambda$ because $\\beta$ is much more interpretable.\n",
    "\n",
    "It is simply the rate/mean/standard deviation of the distribution. $\\beta$ is also known as the **survival parameter** of the distribution, because if the distribution is used to model a process that takes a certain time to occur, it is the mean **survival** time of that process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the uniform distribution\n",
    "\n",
    "The final distribution we will talk about is the **uniform distribution**. It simply describes cases where all values have the exact same frequency. It is a useful distribution because it is used for unbiased sampling (in cases where sampling is necessary).\n",
    "\n",
    "Here's an example uniform distribution over the values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uni = np.random.uniform(size=n_samples)\n",
    "sns.plt.hist(uni,bins=100,alpha=0.5,hold=True,normed=True)\n",
    "print(\"The uniform distribution:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Skewness\n",
    "\n",
    "We are now going to talk about shape properties of all probability distributions. These are the two most common properties that\n",
    "\n",
    "**Skewness** is a measure of how \"skewed\" a distribution is. Right, what is \"skew\"? Really, it measures how asymmetrical a given distribution is around its mean value, and can only be computed for continuous distributions. [skewness wiki](https://en.wikipedia.org/wiki/Skewness)\n",
    "\n",
    "Here are two examples of skewness, demonstrating positive and negative skew:\n",
    "\n",
    "![Skewness](../images/skewness_example.png)\n",
    "\n",
    "Hopefully, you can tell that if these two examples were completely un-skewed, then they would follow the grey dotted lines for the distributions. Positive and negative skew is in reference to the tails of the distributions. **Positive skew** means that the right tail (the part greater than the mean) is bigger than it would be without skew. **Negative skew** means that the left tail (the part of the distribution less than the mean) is bigger than it would be without skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis\n",
    "\n",
    "**Krutosis** is a measure of the \"peakedness\" of a given probability distribution and can only be measured for continuous distributions. [kurtosis wiki](https://en.wikipedia.org/wiki/Kurtosis)\n",
    "\n",
    "![Kurtosis](../images/kurtosis_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
