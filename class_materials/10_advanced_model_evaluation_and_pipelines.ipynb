{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 11 Agenda:\n",
    "  * **Null accuracy**\n",
    "  * **Confusion matrix**\n",
    "  * **sensitivity, specificity, accuracy**\n",
    "  * **ROC curves, AUC, setting a threshold**\n",
    "  * **ROC curves, AUC: All that matters is that ordering is preserved**\n",
    "  * **Pipelines: Putting your entire ML workflow together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to talk about how to more accurately measure models for supervised classification. Because supervised classification problems are ubiquitous (will this person churn? will this ad be clicked? will this stock go up tomorrow?), being able to evaluate how well a supervised classifier works and to be able to choose what defines a \"well-performing\" classifier is very important.\n",
    "\n",
    "To that end, we will also learn how to:\n",
    "  * decompose the kinds of errors a trained model makes (on unseen data)\n",
    "  * decide where (at what probability) to threshold a binary classifier (one that decides between two choices) given what is acceptable in terms of the kinds of errors the model is expected to make in the wild. \n",
    "  * understand confusion matrices\n",
    "  \n",
    "** By the end of this notebook you will:**\n",
    "\n",
    "- Have a working conceptual understanding of key aspects of model evaluation in machine learning\n",
    "- Be able to interpret key model metrics computed using scikit-learn\n",
    "- Be able to to use different model metrics for model evaluation, depending on the goals of your model\n",
    "- Be able to explain what an ROC curve and AUC metric are and how they should be used when evaluating classifiers and setting classifier thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import everything we're going to use today, like always:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data handling, model creation/evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Accuracy\n",
    "\n",
    "Null accuracy measures what our expected accuracy should be if we were to **use the most frequent response (most frequent class) as our prediction for every new sample we see.** It is a baseline against which you may want to measure your classifier, especially when your classes are really unbalanced.\n",
    "\n",
    "In class 5, when we learned about [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression), we attempted to build a predictive model on a dataset that was fairly unbalanced (the vertebral column dataset from UCI).\n",
    "\n",
    "Let's use that dataset again to measure null accuracy, and see whether a basic Logistic Regression model can give higher test set accuracy than this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertebral_data = pd.read_csv(\"../data/vertebral_column_2_categories.dat\",sep=\" \", \n",
    "                             names=[\"pelvic_incidence\",\"pelvic_tilt\",\"lumbar_lordosis_angle\",\n",
    "                                    \"sacral_slope\",\"pelvic_radius\",\"spondy_grade\",\"outcome\"])\n",
    "vertebral_data.outcome.value_counts()\n",
    "vertebral_data.outcome = (vertebral_data.outcome ==\"AB\").astype(int)\n",
    "X = vertebral_data[vertebral_data.columns.tolist()[:-1]]\n",
    "y = vertebral_data.outcome\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "lr = LogisticRegression(C=1e9)\n",
    "lr.fit(X_train,y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Test set accuracy of LR model: \",metrics.accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the null accuracy here?**\n",
    "\n",
    "It is simply the fraction of `AB` (class 1) samples in the test set (since that is the most frequent class in the whole dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute null accuracy manually\n",
    "print(\"Null accuracy on the test set: \",y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute this using the scikit-learn api by creating what's called a `DummyClassifier`.\n",
    "\n",
    "It can create a variety of dummy models based on simple statistics it is trained on.\n",
    "\n",
    "We are going to create one where the model simply predicts the most frequent class, by passing in a `string` to the `strategy` parameter when we generate the unfitted model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dumb_model = DummyClassifier(strategy='most_frequent')\n",
    "dumb_model.fit(X_train, y_train)\n",
    "y_dumb_class = dumb_model.predict(X_test)\n",
    "print(\"Most frequent class dummy classifier test accuracy: \",metrics.accuracy_score(y_test, y_dumb_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time\n",
    "\n",
    "* Use the same dummy classifier approach and train an LR model using 10-fold cross validation and compute the accuracy score of each model.\n",
    "* Do the same with 30-fold cross validation and plot both test-set accuracy distributions. Are the two distributions overlapping? If they aren't what does that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "In order to more fully understand the kinds of mistakes the model is making, we need to investigate what is called the **confusion matrix** of the model on unseen (test) data. The confusion matrix simply counts the number of predictions that fall into each possible prediction bucket.\n",
    "\n",
    "So, for a 2-class classification problem, there are 4 \"prediction buckets\":\n",
    "  * predict 0 (normal), actual 0 (normal) - called a **correct rejection/true negative**\n",
    "  * predict 0 (normal), actual 1 (abnormal) <-- this is an error called a **miss/false negative**\n",
    "  * predict 1 (abnormal), actual 0 (normal) <-- this is an error called a **false alarm/false positive**\n",
    "  * predict 1 (abnormal), actual 1 (abnormal) - called a **hit/true positive**\n",
    "  \n",
    "![confusion matrix](../images/confusion_matrix.png)\n",
    "\n",
    "Let's compute the confusion matrix on the test set for our Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention in a confusion matrix, the actual categories are the rows and the predicted values are the columns. Our confusion matrix shows that on the test set we have an equal number of misses and false alarms (both values are 6).\n",
    "\n",
    "We can break down the confusion matrix into a variety of single-value metrics that answer specific questions about how our model is expected to fare on new data:\n",
    "\n",
    "  * **sensitivity/true positive rate(TPR)/recall:** What fraction of the \"abnormal\" samples in unseen data did we correctly predict? \n",
    "  $$ TPR = \\frac {\\sum TP}{\\sum (TP+FN)}$$\n",
    "  * **specificity/true negative rate(TNR):** What fraction of \"normal\" samples in unseen data did we correctly predict?  \n",
    "  $$ TNR = \\frac {\\sum TN}{\\sum (TN+FP)}$$\n",
    "  * **precision/positive predictive value(PPV)** How frequently is our model correct when it predicts \"abnormal\" on new data?\n",
    "  $$ PPV = \\frac {\\sum TP}{\\sum (TP+FP)}$$\n",
    "  * **negative predictive value (NPV):** How frequently is our model correct when it predicts \"normal\" on new data? \n",
    "  $$ NPV = \\frac {\\sum TN}{\\sum (TN+FN)}$$\n",
    "  * **accuracy (ACC):** How frequently is our model correct on all new data, regardless of class?\n",
    "  $$ ACC = \\frac {\\sum (TN+TP)}{\\sum (TN+FN+TP+FP)}$$\n",
    "  * **F1 score (F1): ** The harmonic mean of precision and recall:\n",
    "  $$ F1 = 2*\\frac {Precision*Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate each metric by hand\n",
    "print(\"Sensitivity/Recall (TPR): \",cm[1,1] / float(cm[1,1] + cm[1,0]))\n",
    "print(\"Specificity (TNR): \", cm[0,0] / float(cm[0,0] + cm[0,1]))\n",
    "print(\"Precision (PPV): \", cm[1,1] / float(cm[1,1]+cm[0,1]))\n",
    "print(\"NPV: \", cm[0,0] / float(cm[0,0]+cm[1,0]))\n",
    "print(\"Accuracy: \", (cm[1,1]+cm[0,0]) / float(cm.sum()))\n",
    "print(\"F1:\", metrics.f1_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate some of these metrics using sklearn and the test set samples\n",
    "print(\"Sensitivity/Recall (TPR): \",metrics.recall_score(y_test,y_test_pred))\n",
    "print(\"Precision (PPV): \", metrics.precision_score(y_test,y_test_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_test_pred))\n",
    "print(\"F1:\", metrics.f1_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or we can compute the full classification report, which will give us precision/recall per-feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and AUC\n",
    "\n",
    "So far, we've working with the thresholded decisions a given classifier/model makes when it outputs a prediction (all we've been looking at is the predicted category of trained models). \n",
    "\n",
    "However, many classifiers (including both **logistic regression** and **random forest** models) can output a \"confidence\" associated with their prediction (this is called a **prediction probability**). \n",
    "\n",
    "Let's take a look at the prediction probabilites and the predictions of our original logistic regression classifier on our single test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lr probabilities per category for first five samples\n",
    "predicted_probs_lr = lr.predict_proba(X_test).round(3)\n",
    "predictions_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression predicted probabilities for first five samples in test set:\\n\",predicted_probs_lr[:5])\n",
    "print(\"Logistic Regression predictions for first five samples in test set:\\n\",predictions_lr[:5])\n",
    "y_test_lr_df = pd.DataFrame(\n",
    "    np.concatenate((\n",
    "        predicted_probs_lr,predictions_lr.reshape((predictions_lr.shape[0],-1)),\n",
    "        y_test.reshape((y_test.shape[0],-1))),axis=1\n",
    "    ),\n",
    "    columns = [\"class_0\",\"class_1\",\"predicted\",\"actual\"])\n",
    "\n",
    "y_test_lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate the same table of predicted probabilities, predictions, and actual values, for a trained random forest classifier that contains 100 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "predicted_probs_rf = rf.predict_proba(X_test)\n",
    "predictions_rf = rf.predict(X_test)\n",
    "\n",
    "y_test_rf_df = pd.DataFrame(\n",
    "    np.concatenate((\n",
    "        predicted_probs_rf,predictions_rf.reshape((predictions_rf.shape[0],-1)),\n",
    "        y_test.reshape((y_test.shape[0],-1))),axis=1\n",
    "    ),\n",
    "    columns = [\"class_0\",\"class_1\",\"predicted\",\"actual\"])\n",
    "\n",
    "y_test_rf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of these cases, the prediction threshold (for either class) is set to 0.5, so the class with the probability that is over 0.5 is the predicted class.\n",
    "\n",
    "**However, 0.5 doesn't necessarily have to be the threshold we use for our classifier's decision threshold!**\n",
    "\n",
    "For example, we might want to be VERY CERTAIN that the classifier thinks something is class 1 (for example, a likelihood of cancer presence given some test results). \n",
    "\n",
    "In that case, we would require that the class 1 probability is > 0.8 (for example). As a result, the ratio of TP/TN/FN/FP would change, yielding a new confusion matrix, with a new set of precision/recall/accuracy estimates.\n",
    "\n",
    "**In general, the default probability threshold for a given classifier maximizes accuracy, but not the other metrics like precision, recall, etc.**\n",
    "\n",
    "But, what if we varied the prediction thresholds to obtain different TP/TN/FP/FN values (different confusion matrices) across all possible threshold values for our classifier?\n",
    "\n",
    "This is exactly what an [ROC (Receiver Operating Characteristic) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) does.\n",
    "\n",
    "**An ROC Curve is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is systematically varied.**\n",
    "\n",
    "Let's generate the ROC curves for both of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate lr model false positive and true positive rates\n",
    "fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(y_test, predicted_probs_lr[:,1])\n",
    "\n",
    "#generate same for random forest model\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(y_test, predicted_probs_rf[:,1])\n",
    "\n",
    "# plot LR and RF model ROC curves\n",
    "sns.plt.plot(fpr_lr, tpr_lr,label=\"lr\")\n",
    "sns.plt.plot(fpr_rf, tpr_rf,label=\"rf\")\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.legend(loc=\"lower right\")\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve allows you to balance how many FN's (misses) and FP's (false alarms) you're willing to have with respect to one of your classes.\n",
    "\n",
    "There's a metric that allows you to quantify the overall performance of your binary classifier, regardless of the threshold you choose. This metric is called the **AUC (area under the curve)**, and is a systematic way to compare two classifiers relative to each other, across all decision thresholds. If model A has a higher AUC than model B then this means that on average, model A is a better classifier across all decision thresholds (all probabilities). \n",
    "\n",
    "Besides allowing you to calculate AUC, seeing the ROC curve can help you to choose a threshold that **balances sensitivity and specificity** in a way that makes sense for the particular context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate AUC for lr and rf\n",
    "print(\"LR model AUC: \",metrics.roc_auc_score(y_test, predicted_probs_lr[:,1]))\n",
    "print(\"RF model AUC: \",metrics.roc_auc_score(y_test, predicted_probs_rf[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this more explicit. Let's say instead of using 0.5 as the probability threshold for class 1, we want the probability threshold where the false positive rate is no higher than 12% (because additional false positives cost too much money, or time, etc.). \n",
    "\n",
    "Let's take a look at a graph of the ROC curve and the class 1 thresholds as a function of FPR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot LR and RF model ROC curves\n",
    "sns.plt.plot(fpr_lr, tpr_lr,label=\"lr\")\n",
    "sns.plt.plot(fpr_lr,thresholds_lr, label=\"lr_thresh\")\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.legend(loc=\"center\")\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity) or Class 1 Threshold Probability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the threshold that satisfies our criteria is ~0.7. Let's use this new threshold to generate our predictions and see what our new confusion matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_lr_df[\"predicted_07\"] = (y_test_lr_df.class_1 > 0.7).astype(float)\n",
    "print(y_test_lr_df.head())\n",
    "print(\"Confusion matrix at original 0.5 threshold:\\n\",metrics.confusion_matrix(y_test_lr_df.actual,\n",
    "                                                                      y_test_lr_df.predicted),\"\\n\")\n",
    "print(\"Classification Report at original 0.5 threshold:\\n\", metrics.classification_report(y_test_lr_df.actual,\n",
    "                                                                                          y_test_lr_df.predicted),\"\\n\")\n",
    "print(\"Confusion matrix at 0.7 threshold:\\n\",metrics.confusion_matrix(y_test_lr_df.actual,\n",
    "                                                                      y_test_lr_df.predicted_07),\"\\n\")\n",
    "print(\"Classification Report at 0.7 threshold:\\n\", metrics.classification_report(y_test_lr_df.actual,\n",
    "                                                                                 y_test_lr_df.predicted_07))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we've now increased our precision for class 1 (we are more certain someone has a back problem, when our classifier says you have a back problem), but have also lowered our recall for class 1 (now, more people with back problems will go un-identified by our classifier). The opposite occurs for precision and recall for class 0 as a result.\n",
    "\n",
    "**So, depending on what you want to maximize or minimize for your machine learning application, your decision threshold for your (completely trained) classifier will differ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One final important point about computing the AUC, you have to use the predicted probabilities, not the class labels themselves when computing an ROC curve or AUC.**\n",
    "\n",
    "If you use y_pred_class, it will not give you an error, rather it will interpret the ones and zeros as predicted probabilities of 100% and 0%, and thus will give you incorrect results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate AUC using y_pred_class (producing incorrect results)\n",
    "print(\"Wrong way to calculate LR model AUC: \",metrics.roc_auc_score(y_test, predictions_lr))\n",
    "print(\"Wrong way to calculate RF model AUC: \",metrics.roc_auc_score(y_test, predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final way to compare the two models, let's see if we can look at each of their predicted probabilites on the test set as a function of the actual category (the target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# histogram of predicted probabilities grouped by actual response value for LR\n",
    "y_test_lr_df.class_1.hist(by= y_test_lr_df.actual, sharex=True, sharey=True)\n",
    "#same for RF\n",
    "y_test_rf_df.class_1.hist(by= y_test_rf_df.actual, sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what threshold we choose will affect what part of each histogram consists of misclassified examples.\n",
    "\n",
    "Nonetheless, it is fruitful at this point to examine all of those test-set samples that each classifier was \"very certain\" about (had very high or very low probability) but was of the wrong category (a class 0 sample that had a very high probability or a class 1 sample with a probability very close to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All that matters for ROC/AUC is the order of the probabilities, not their values\n",
    "\n",
    "Let's go through our vertebral data example from start to finish to demonstrate a very important point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load in data\n",
    "vertebral_data = pd.read_csv(\"../data/vertebral_column_2_categories.dat\",sep=\" \", \n",
    "                             names=[\"pelvic_incidence\",\"pelvic_tilt\",\"lumbar_lordosis_angle\",\n",
    "                                    \"sacral_slope\",\"pelvic_radius\",\"spondy_grade\",\"outcome\"])\n",
    "\n",
    "#convert outcome into binary 0/1 attribute\n",
    "le = LabelEncoder()\n",
    "vertebral_data.outcome = le.fit_transform(vertebral_data.outcome)\n",
    "X = vertebral_data[vertebral_data.columns.tolist()[:-1]]\n",
    "y = vertebral_data.outcome\n",
    "\n",
    "#create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "#create logistic regression object\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Test set accuracy of default 0.5 threshold LR model: \",metrics.accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for class 1\n",
    "y_pred_prob1 = lr.predict_proba(X_test)[:, 1]\n",
    "# show predicted probabilities in a histogram\n",
    "sns.plt.hist(y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob1)\n",
    "sns.plt.plot(fpr, tpr)\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Demonstration: Take the square root of predicted probabilities (makes them all bigger, but preserve the order of probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the predicted probabilities\n",
    "y_pred_prob2 = np.sqrt(y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here are the old ones (y_pred_prob1)\n",
    "print(\"Old predicted probs:\\n\",y_pred_prob1[:10].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here are the new ones (y_pred_prob2)\n",
    "print(\"New predicted probs:\\n\",y_pred_prob2[:10].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can see the histogram changed\n",
    "figure = sns.plt.figure(figsize=(12,8))\n",
    "figure.add_subplot(121)\n",
    "sns.plt.title(\"Original histogram of predicted probabilities\")\n",
    "sns.plt.hist(y_pred_prob1)\n",
    "figure.add_subplot(122)\n",
    "sns.plt.title(\"Histogram of square root predicted probabilities\")\n",
    "sns.plt.hist(y_pred_prob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the AUC did not change\n",
    "print(\"Old AUC: \",metrics.roc_auc_score(y_test, y_pred_prob1))\n",
    "print(\"New AUC: \",metrics.roc_auc_score(y_test, y_pred_prob2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the ROC curve did not change\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(y_test, y_pred_prob2)\n",
    "figure = sns.plt.figure(figsize=(12,8))\n",
    "figure.add_subplot(121)\n",
    "sns.plt.plot(fpr, tpr)\n",
    "sns.plt.title(\"Original ROC Curve\")\n",
    "figure.add_subplot(122)\n",
    "sns.plt.title(\"ROC Curve of sqrt probabilities\")\n",
    "sns.plt.plot(fpr2, tpr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make small predicted probabilities smaller, and make big predicted probabilities bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob3 = np.where(y_pred_prob1 > 0.5, np.sqrt(y_pred_prob1), y_pred_prob1**2)\n",
    "# you can see these are different from y_pred_prob1 and y_pred_prob2\n",
    "y_pred_prob3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the histogram changed\n",
    "# you can see the histogram changed\n",
    "figure = sns.plt.figure(figsize=(12,8))\n",
    "figure.add_subplot(131)\n",
    "sns.plt.title(\"Original histogram\")\n",
    "sns.plt.hist(y_pred_prob1)\n",
    "figure.add_subplot(132)\n",
    "sns.plt.title(\"Histogram of square root pred probs\")\n",
    "sns.plt.hist(y_pred_prob2)\n",
    "figure.add_subplot(133)\n",
    "sns.plt.title(\"Histogram of newest pred probs\")\n",
    "sns.plt.hist(y_pred_prob3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the AUC did not change\n",
    "print(\"Original AUC: \",metrics.roc_auc_score(y_test, y_pred_prob1))\n",
    "print(\"Square root probs AUC: \",metrics.roc_auc_score(y_test, y_pred_prob2))\n",
    "print(\"Newest transformed AUC: \",metrics.roc_auc_score(y_test, y_pred_prob3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr3, tpr3, thresholds2 = metrics.roc_curve(y_test, y_pred_prob3)\n",
    "# the ROC curve did not change\n",
    "figure = sns.plt.figure(figsize=(12,8))\n",
    "figure.add_subplot(131)\n",
    "sns.plt.plot(fpr, tpr)\n",
    "sns.plt.title(\"Original ROC Curve\")\n",
    "figure.add_subplot(132)\n",
    "sns.plt.title(\"ROC Curve of sqrt probs\")\n",
    "sns.plt.plot(fpr2, tpr2)\n",
    "figure.add_subplot(133)\n",
    "sns.plt.title(\"ROC Curve of newest transformed probs\")\n",
    "sns.plt.plot(fpr3, tpr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of all of this is, as long as the ordering of probabilities for all of your samples is maintained, the ROC will remain identical.\n",
    "\n",
    "All that matters is the order of the predicted probabilities in predicted data, not the actual values!\n",
    "\n",
    "### ROC/Confusion Matrix/Metric Takeaways\n",
    "\n",
    "* For binary classifiers, you want to have the highest AUC possible. To decide between two classifiers, maximize the one that has the higher AUC.\n",
    "* Once you've maximized your AUC, you set about choosing your classifier threshold based on the specifics of your problem. If you need to minimize false alarms (so maximize precision) while keeping your recall some acceptable level, then you do one thing. If you need to maximize accuracy, you do something else.\n",
    "* In every case, what you're doing is generating a threshold from a specific point on the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines: Putting An Entire Model Together End to End\n",
    "\n",
    "Ok, the last thing we are going to learn how to do, is how to combine every aspect creating and using a supervised machine learning model:\n",
    "\n",
    "1. Transforming your original data (removing skew, standard scaling, encoding categorical variables as numbers)\n",
    "2. Training and validating a model on that data\n",
    "3. Picking parameters for a given model to optimize accuracy/precision/recall/f1 score, etc.\n",
    "\n",
    "Let's try to see how we would do this without a pipeline. Let's get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "numeric_columns = columns[1:-1]\n",
    "categorical_columns = columns[0]\n",
    "target = columns[-1]\n",
    "\n",
    "abalone_data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "abalone_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's preprocess it in the standard way I've shown you:\n",
    "\n",
    "1. Let's convert the categorical column using one-hot encoding\n",
    "2. Standard scale (Z-score) the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get categorical features\n",
    "#drop off last column because its unnecessary\n",
    "X_categorical = pd.get_dummies(abalone_data[categorical_columns]).astype(int).ix[:,:-1]\n",
    "\n",
    "#get and transform numeric features\n",
    "X_numeric = abalone_data[numeric_columns]\n",
    "X_numeric[numeric_columns] = StandardScaler().fit_transform(X_numeric)\n",
    "\n",
    "#get outcome variable\n",
    "y = abalone_data[target]\n",
    "\n",
    "#combine transformed categorical and numeric features\n",
    "X_final = pd.concat((X_numeric,X_categorical),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's do our standard 10-fold cross-validation scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create rf regressor and check 10-fold RMSE\n",
    "rf = RandomForestRegressor()\n",
    "cross_val_scores = np.abs(cross_val_score(rf,X_final,y,scoring = \"mean_squared_error\", cv=10))\n",
    "rmse_cross_val_scores = np.sqrt(cross_val_scores)\n",
    "print(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))\n",
    "print(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to do the same thing using Scikit-learn's pipeline feature. First, we are going to have a class that allows us to subselect columns that we want to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to make the full pipeline, from start to finish, for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#encode the categorical column from strings to ints\n",
    "le = LabelEncoder()\n",
    "abalone_data[\"sex_encoded\"] = abalone_data[[categorical_columns]].apply(le.fit_transform)\n",
    "\n",
    "#extract the y\n",
    "y = abalone_data.age\n",
    "\n",
    "#create the feature union for the features\n",
    "X_transformed_pipe = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for one hot encoding categorical column\n",
    "            ('sexes', Pipeline([\n",
    "                ('selector', ItemSelector(key=[\"sex_encoded\"])),\n",
    "                ('encoder', OneHotEncoder())                    \n",
    "            ])),\n",
    "            # Pipeline for pulling out numeric features and scaling them\n",
    "            ('numeric', Pipeline([\n",
    "                ('selector', ItemSelector(key=numeric_columns)),\n",
    "                #('polyfeatures', PolynomialFeatures(degree=2,interaction_only=True)),\n",
    "                ('scaler', StandardScaler()),\n",
    "            ]))])\n",
    "#create the full final pipeline\n",
    "full_pipeline = Pipeline([(\"all_features\",X_transformed_pipe),(\"rf_regressor\",RandomForestRegressor(n_estimators=100))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run the whole pipe through the `cross_val_score` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pass the pipeline directly into cross_val_score\n",
    "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\n",
    "rmse_cross_val_scores = np.sqrt(cross_val_scores)\n",
    "print(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))\n",
    "print(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!\n",
    "\n",
    "* Change the pipeline to perform PCA and keep only the first 6 components on the complete feature pipe (after standard scaling numeric features and encoding the categorical feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this on a slightly more involved example, where we will have to do some imputation (filling in of missing values).\n",
    "\n",
    "Here the process will be as follows:\n",
    "\n",
    "1. Encode categorical string columns as numbers using `LabelEncoder`\n",
    "2. Impute missing categorical values (marked with 0 after encoding) with most frequent category using `Imputer`\n",
    "3. One-hot encode the categorical columns using `OneHotEncoder`\n",
    "4. Impute missing numerical values using the median value of each column using `Imputer`\n",
    "5. Z-score/standardize each numerica column using `StandardScaler`\n",
    "6. Combine both collections of columns (one-hot encoded categorical columns and standardized numeric columns) using `FeatureUnion`\n",
    "7. Pass the whole collection to a `RandomForestClassifier` to build a Random Forest classification model.\n",
    "8. Use `cross_val_score` with 10-fold cross-validation on the entire pipeline.\n",
    "\n",
    "Ready? Let's start by loading in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kidney_columns = [\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"class\"]\n",
    "kidney_data = pd.read_csv(\"../data/chronic_kidney_disease.csv\",header=None,na_values=\"?\",names=kidney_columns)\n",
    "kidney_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange the columns so that the numeric columns are together, followed by all of the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rearrange kidney columns as before\n",
    "kidney_columns = kidney_columns[:5]+kidney_columns[9:18]+kidney_columns[5:9]+kidney_columns[18:]\n",
    "kidney_data = kidney_data[kidney_columns]\n",
    "kidney_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the strings as numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kidney_columns[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kidney_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert strings to numbers\n",
    "le = LabelEncoder()\n",
    "kidney_data[kidney_columns[14:]] = kidney_data[kidney_columns[14:]].fillna(\"\").apply(le.fit_transform)\n",
    "#get the X and y\n",
    "X = kidney_data[kidney_columns[:-1]]\n",
    "y = kidney_data[\"class\"]\n",
    "kidney_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code for the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X_transformed_pipe = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for filling in missing values, one hot encoding all categorical columns\n",
    "            ('categoricals', Pipeline([\n",
    "                ('selector', ItemSelector(key=kidney_columns[14:-1])),\n",
    "                ('imputer', Imputer(missing_values=0,strategy=\"most_frequent\",axis=0)),\n",
    "                ('encoder', OneHotEncoder())                    \n",
    "            ])),\n",
    "            # Pipeline for pulling out numeric features, filling in missing values, and scaling them\n",
    "            ('numeric', Pipeline([\n",
    "                ('selector', ItemSelector(key=kidney_columns[:14])),\n",
    "                ('imputer', Imputer(strategy=\"median\",axis=0)),\n",
    "                ('scaler', StandardScaler()),\n",
    "            ]))])\n",
    "\n",
    "full_pipeline = Pipeline([(\"all_features\",X_transformed_pipe),(\"rf_classifier\",RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(full_pipeline,X,y,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pipeline object contains a sequence of steps, which are stored in a list. Each step is a tuple, where the first element is the name you gave the given step, and the second element is the transformation or model you are applying at that step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipeline.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The first step in the pipeline:\\n\",full_pipeline.steps[0])\n",
    "print()\n",
    "print(\"The second step in the pipeline: \\n\", full_pipeline.steps[1])\n",
    "print()\n",
    "print(\"The second step's transformation/model:\\n\", full_pipeline.steps[1][1])\n",
    "print(\"Since we know this is a random forest model, lets try to get the models feature importances:\\n\",full_pipeline.steps[1][1].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, in order to be able to get feature importances or coefficients of a given model, it needs to be trained first. Just like any other transformation in sklearn, you can fit a pipeline by calling its `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's been fit, we can extract the feature importances as we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipeline.steps[1][1].feature_importances_.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's really great about pipelines is that you can even put them into `GridSearchCV` methods, and search across parameters to tune your models. To do so, create a dictionary entry in `param_grid` that names the step (which you named earlier) and parameters you want to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using GridSearchCV with Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "estimators_range = [20,50,100]\n",
    "param_grid = dict(rf_classifier__n_estimators=estimators_range)\n",
    "grid = GridSearchCV(full_pipeline, param_grid, cv=20, scoring='accuracy',n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "print(\"Best cross-validated accuracy: \",grid.best_score_)\n",
    "print(\"Best parameter found: \",grid.best_params_)\n",
    "print(\"Fitted_model: \",grid.best_estimator_.steps[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!\n",
    "\n",
    "* add a PCA transformation step before training the classifier\n",
    "* search over the number of PCA components to keep using `GridSearchCV` (test whether to keep the first 5,10, or all components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
