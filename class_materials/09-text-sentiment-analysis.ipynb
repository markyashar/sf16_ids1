{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "try:\n",
    "    from sklearn.model_selection import cross_val_score, train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "#data handling/modeling\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = [\"These debates are boring\", \"we want more debates\", \"debates are useful\"]\n",
    "target = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "# Init signature: CountVectorizer(self, input=u'content', encoding=u'utf-8', decode_error=u'strict', \n",
    "#                                 strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "#                                 stop_words=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), \n",
    "#                                 analyzer=u'word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, \n",
    "#                                 binary=False, dtype=<type 'numpy.int64'>)\n",
    "# Docstring:\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "# This implementation produces a sparse representation of the counts using\n",
    "# scipy.sparse.coo_matrix.\n",
    "\n",
    "# If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "# that does some kind of feature selection then the number of features will\n",
    "# be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "# stop_words : string {'english'}, list, or None (default)\n",
    "#     If 'english', a built-in stop word list for English is used.\n",
    "\n",
    "#     If a list, that list is assumed to contain stop words, all of which\n",
    "#     will be removed from the resulting tokens.\n",
    "#     Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "#     If None, no stop words will be used. max_df can be set to a value\n",
    "#     in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "#     words based on intra corpus document frequency of terms.\n",
    "\n",
    "\n",
    "tweet_X = vect.fit_transform(tweets)\n",
    "print(tweet_X.shape)\n",
    "print(tweet_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'boring', u'debates', u'useful', u'want']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boring</th>\n",
       "      <th>debates</th>\n",
       "      <th>useful</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   boring  debates  useful  want\n",
       "0       1        1       0     0\n",
       "1       0        1       0     1\n",
       "2       0        1       1     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tweet_X.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(path, target):\n",
    "    reviews = []\n",
    "    for file in glob(path):\n",
    "        review = open(file).read()\n",
    "        reviews.append({\n",
    "                \"target\": target,\n",
    "                \"review\": review\n",
    "            })\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = load_data(\"../data/review_polarity/txt_sentoken/neg/*\", \"neg\") + \\\n",
    "    load_data(\"../data/review_polarity/txt_sentoken/pos/*\", \"pos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "<bound method DataFrame.head of                                                  review target\n",
      "0     plot : two teen couples go to a church party ,...    neg\n",
      "1     the happy bastard's quick movie review \\ndamn ...    neg\n",
      "2     it is movies like these that make a jaded movi...    neg\n",
      "3      \" quest for camelot \" is warner bros . ' firs...    neg\n",
      "4     synopsis : a mentally unstable man undergoing ...    neg\n",
      "5     capsule : in 2176 on the planet mars police ta...    neg\n",
      "6     so ask yourself what \" 8mm \" ( \" eight millime...    neg\n",
      "7     that's exactly how long the movie felt to me ....    neg\n",
      "8     call it a road trip for the walking wounded . ...    neg\n",
      "9     plot : a young french boy sees his parents kil...    neg\n",
      "10    best remembered for his understated performanc...    neg\n",
      "11    janeane garofalo in a romantic comedy -- it wa...    neg\n",
      "12    and now the high-flying hong kong style of fil...    neg\n",
      "13    a movie like mortal kombat : annihilation work...    neg\n",
      "14    she was the femme in \" la femme nikita . \" \\nh...    neg\n",
      "15    john carpenter makes b-movies . \\nalways has (...    neg\n",
      "16    i'm really starting to wonder about alicia sil...    neg\n",
      "17    so what do you get when you mix together plot ...    neg\n",
      "18    the law of crowd pleasing romantic movies stat...    neg\n",
      "19    mighty joe young blunders about for nearly twe...    neg\n",
      "20     \" spawn \" features good guys , bad guys , lot...    neg\n",
      "21     \" in dreams \" might keep you awake at night ,...    neg\n",
      "22     \" knock off \" is exactly that : a cheap knock...    neg\n",
      "23     \" snake eyes \" is the most aggravating kind o...    neg\n",
      "24    forgive the fevered criticism but the fervor o...    neg\n",
      "25    one might expect a cathartic viewing experienc...    neg\n",
      "26    america loves convenience . \\nafter all , we'r...    neg\n",
      "27    reindeer games is easily the worst of the thre...    neg\n",
      "28    a follow-up to disney's live-action \" 101 dalm...    neg\n",
      "29    one-sided \" doom and gloom \" documentary about...    neg\n",
      "...                                                 ...    ...\n",
      "1970  synopsis : in phantom menace the galaxy is div...    pos\n",
      "1971  at one point during brian de palma's crime epi...    pos\n",
      "1972  for any groom on the verge of proposing marria...    pos\n",
      "1973  i like movies with albert brooks , and i reall...    pos\n",
      "1974   ( note : there are spoilers regarding the fil...    pos\n",
      "1975  martin scorsese's films used to intimidate me ...    pos\n",
      "1976  robert redford's a river runs through it is no...    pos\n",
      "1977  richard gere is not one of my favorite actors ...    pos\n",
      "1978   \" when you get out of jail , you can kill him...    pos\n",
      "1979  let's say you live at the end of an airport ru...    pos\n",
      "1980  in this good natured , pleasent and easy going...    pos\n",
      "1981   \" no man is an island , \" one character quote...    pos\n",
      "1982  i rented this movie with very high hopes . \\nt...    pos\n",
      "1983  few films in 1999 have divided the critical co...    pos\n",
      "1984  now that \" boogie nights \" has made disco resp...    pos\n",
      "1985  this has been an extraordinary year for austra...    pos\n",
      "1986  i think the first thing this reviewer should m...    pos\n",
      "1987  trees lounge is the directoral debut from one ...    pos\n",
      "1988  i wish i could say that there is something mor...    pos\n",
      "1989  lisa cholodenko's \" high art , \" is an intelli...    pos\n",
      "1990  the relaxed dude rides a roller coaster \\nthe ...    pos\n",
      "1991  i don't box with kid gloves . \\ni don't play n...    pos\n",
      "1992  here is a film that is so unexpected , so scar...    pos\n",
      "1993  plot : this movie takes place over one day . \\...    pos\n",
      "1994  a thriller set in modern day seattle , that ma...    pos\n",
      "1995  wow ! what a movie . \\nit's everything a movie...    pos\n",
      "1996  richard gere can be a commanding actor , but h...    pos\n",
      "1997  glory--starring matthew broderick , denzel was...    pos\n",
      "1998  steven spielberg's second epic film on world w...    pos\n",
      "1999  truman ( \" true-man \" ) burbank is the perfect...    pos\n",
      "\n",
      "[2000 rows x 2 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>senseless ( r ) marlon wayans is a very talent...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>robin williams is a comedic genus . \\nthat is ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>it would be hard to choose the best american p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>if you've ever perused my college comedy diary...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>everyone's heard about this movie , and more s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>susan granger's review of \" america's sweethea...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>studio 54 attracted so many weird and bizarre ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>who would have thought ? \\njim carrey does dra...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>one of the most respected names in american in...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>as a hot-shot defense attorney , kevin lomax (...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review target\n",
       "156   senseless ( r ) marlon wayans is a very talent...    neg\n",
       "163   robin williams is a comedic genus . \\nthat is ...    neg\n",
       "278   it would be hard to choose the best american p...    neg\n",
       "1664  if you've ever perused my college comedy diary...    pos\n",
       "1602  everyone's heard about this movie , and more s...    pos\n",
       "196   susan granger's review of \" america's sweethea...    neg\n",
       "321   studio 54 attracted so many weird and bizarre ...    neg\n",
       "1695  who would have thought ? \\njim carrey does dra...    pos\n",
       "60    one of the most respected names in american in...    neg\n",
       "333   as a hot-shot defense attorney , kevin lomax (...    neg"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.head)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968    while watching loser , it occurred to me that ...\n",
      "240    georges polti once wrote a paper called \" the ...\n",
      "819    sylvester stallone has made some crap films in...\n",
      "692    attention moviegoers : you are about to enter ...\n",
      "420    plot : something about a bunch of kids going i...\n",
      "Name: review, dtype: object\n",
      "(1600,)\n",
      "(400,)\n",
      "(1600,)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')  # instantiate the model\n",
    "X_train_vect = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600x35944 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 390595 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()   # define the logistic regression\n",
    "logreg.fit(X_train_vect, y_train)   # we fit it\n",
    "# outcome_pred_class_log = logreg.predict(X_test)   # we make (class) predictions based on the data that we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_vect = vect.transform(X_test)   # this is an important step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83250000000000002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n",
      "['pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos'\n",
      " 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg'\n",
      " 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos' 'neg'\n",
      " 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'pos'\n",
      " 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg'\n",
      " 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg'\n",
      " 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg'\n",
      " 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg'\n",
      " 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg'\n",
      " 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg'\n",
      " 'pos' 'pos' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "y_pred=logreg.predict(X_test_vect)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164,  35],\n",
       "       [ 32, 169]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "confusion_matrix(y_test, y_pred)   # What is the confusion matrix telling us here? Instructor explained but\n",
    "                                   # I seemed to have missed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos'\n",
      " 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg'\n",
      " 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos' 'neg'\n",
      " 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'pos'\n",
      " 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg'\n",
      " 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg'\n",
      " 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg'\n",
      " 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos'\n",
      " 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'neg' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'neg' 'neg' 'pos' 'pos' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg'\n",
      " 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'neg' 'neg' 'pos' 'pos' 'pos' 'neg'\n",
      " 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'pos'\n",
      " 'pos' 'neg' 'pos' 'pos' 'neg' 'pos' 'pos' 'pos' 'neg' 'neg' 'neg' 'neg'\n",
      " 'pos' 'pos' 'neg' 'neg']\n",
      "1860    pos\n",
      "353     neg\n",
      "1333    pos\n",
      "905     neg\n",
      "1289    pos\n",
      "1273    pos\n",
      "938     neg\n",
      "1731    pos\n",
      "65      neg\n",
      "1323    pos\n",
      "56      neg\n",
      "1292    pos\n",
      "1118    pos\n",
      "584     neg\n",
      "374     neg\n",
      "275     neg\n",
      "746     neg\n",
      "128     neg\n",
      "1646    pos\n",
      "1852    pos\n",
      "674     neg\n",
      "1664    pos\n",
      "1981    pos\n",
      "1083    pos\n",
      "1922    pos\n",
      "99      neg\n",
      "1179    pos\n",
      "964     neg\n",
      "792     neg\n",
      "29      neg\n",
      "       ... \n",
      "1543    pos\n",
      "1803    pos\n",
      "67      neg\n",
      "1737    pos\n",
      "1592    pos\n",
      "1563    pos\n",
      "480     neg\n",
      "1756    pos\n",
      "1912    pos\n",
      "1440    pos\n",
      "1460    pos\n",
      "887     neg\n",
      "422     neg\n",
      "1674    pos\n",
      "1449    pos\n",
      "764     neg\n",
      "519     neg\n",
      "1431    pos\n",
      "613     neg\n",
      "1530    pos\n",
      "1782    pos\n",
      "1658    pos\n",
      "453     neg\n",
      "730     neg\n",
      "534     neg\n",
      "965     neg\n",
      "1284    pos\n",
      "1739    pos\n",
      "261     neg\n",
      "535     neg\n",
      "Name: target, dtype: object\n",
      "  (0, 709)\t1\n",
      "  (0, 773)\t1\n",
      "  (0, 837)\t1\n",
      "  (0, 878)\t1\n",
      "  (0, 921)\t1\n",
      "  (0, 1134)\t1\n",
      "  (0, 1348)\t1\n",
      "  (0, 1536)\t1\n",
      "  (0, 1719)\t5\n",
      "  (0, 1834)\t1\n",
      "  (0, 1840)\t1\n",
      "  (0, 1999)\t1\n",
      "  (0, 2051)\t2\n",
      "  (0, 2953)\t3\n",
      "  (0, 2988)\t8\n",
      "  (0, 3113)\t1\n",
      "  (0, 3186)\t1\n",
      "  (0, 3206)\t1\n",
      "  (0, 3383)\t1\n",
      "  (0, 3546)\t1\n",
      "  (0, 3557)\t1\n",
      "  (0, 3705)\t1\n",
      "  (0, 3953)\t1\n",
      "  (0, 4097)\t1\n",
      "  (0, 4349)\t2\n",
      "  :\t:\n",
      "  (399, 30620)\t1\n",
      "  (399, 30750)\t1\n",
      "  (399, 30864)\t2\n",
      "  (399, 30928)\t1\n",
      "  (399, 30968)\t1\n",
      "  (399, 32115)\t1\n",
      "  (399, 32267)\t1\n",
      "  (399, 32809)\t1\n",
      "  (399, 32948)\t1\n",
      "  (399, 33207)\t1\n",
      "  (399, 33452)\t1\n",
      "  (399, 33471)\t1\n",
      "  (399, 33819)\t1\n",
      "  (399, 34034)\t1\n",
      "  (399, 34040)\t1\n",
      "  (399, 34061)\t1\n",
      "  (399, 34788)\t1\n",
      "  (399, 34789)\t1\n",
      "  (399, 34792)\t1\n",
      "  (399, 34913)\t2\n",
      "  (399, 35009)\t1\n",
      "  (399, 35378)\t1\n",
      "  (399, 35430)\t1\n",
      "  (399, 35544)\t1\n",
      "  (399, 35783)\t1\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 35944)\n",
      "  (0, 13169)\t0.0410632747207\n",
      "  (0, 3997)\t0.0328509013804\n",
      "  (0, 7413)\t0.0386364198572\n",
      "  (0, 7776)\t0.0416695334775\n",
      "  (0, 25002)\t0.0365302536074\n",
      "  (0, 13350)\t0.033408605374\n",
      "  (0, 15525)\t0.053547502712\n",
      "  (0, 4083)\t0.053547502712\n",
      "  (0, 10886)\t0.0471631734538\n",
      "  (0, 14283)\t0.0637076695209\n",
      "  (0, 15517)\t0.0326744975201\n",
      "  (0, 9552)\t0.0487179440674\n",
      "  (0, 21275)\t0.0487179440674\n",
      "  (0, 16827)\t0.0286764969936\n",
      "  (0, 35377)\t0.029080527868\n",
      "  (0, 12789)\t0.053547502712\n",
      "  (0, 8420)\t0.0344504797388\n",
      "  (0, 19738)\t0.0228859477727\n",
      "  (0, 14341)\t0.0202193617644\n",
      "  (0, 18262)\t0.0423336148093\n",
      "  (0, 7815)\t0.0430677226632\n",
      "  (0, 2733)\t0.0336048882216\n",
      "  (0, 31894)\t0.0405055707271\n",
      "  (0, 718)\t0.0471631734538\n",
      "  (0, 28225)\t0.0218179997652\n",
      "  :\t:\n",
      "  (1599, 14341)\t0.0231149680283\n",
      "  (1599, 23939)\t0.0176607068953\n",
      "  (1599, 17848)\t0.0314867107273\n",
      "  (1599, 9490)\t0.0296897872448\n",
      "  (1599, 33124)\t0.0227247477411\n",
      "  (1599, 32110)\t0.0351210236127\n",
      "  (1599, 15444)\t0.0695457137845\n",
      "  (1599, 29230)\t0.0399153421782\n",
      "  (1599, 15344)\t0.0243762385249\n",
      "  (1599, 15451)\t0.034124104905\n",
      "  (1599, 17435)\t0.0112575330603\n",
      "  (1599, 5473)\t0.0133143439618\n",
      "  (1599, 8798)\t0.0208904005086\n",
      "  (1599, 19361)\t0.0192965616453\n",
      "  (1599, 12912)\t0.0189138263656\n",
      "  (1599, 31586)\t0.02626177427\n",
      "  (1599, 6394)\t0.0237747903233\n",
      "  (1599, 10252)\t0.0417617358074\n",
      "  (1599, 3451)\t0.0168315569429\n",
      "  (1599, 13303)\t0.017015382129\n",
      "  (1599, 23180)\t0.0275475772652\n",
      "  (1599, 19357)\t0.0161916549895\n",
      "  (1599, 20984)\t0.0599656807306\n",
      "  (1599, 30307)\t0.0195694480398\n",
      "  (1599, 11988)\t0.0270338612286\n"
     ]
    }
   ],
   "source": [
    "# The word 'movie' adds noise because it occurs in all sentences in the review and is therefore useless.\n",
    "vect = TfidfVectorizer(stop_words='english')  # instantiate the model\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "print(X_train_vect.shape)\n",
    "print(X_train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest(k=2000, score_func=<function f_classif at 0x11725df50>)\n"
     ]
    }
   ],
   "source": [
    "kbest = SelectKBest(k=2000)\n",
    "print(kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n",
      "  (0, 1946)\t0.0179293172722\n",
      "  (0, 1827)\t0.0193954605291\n",
      "  (0, 643)\t0.0241477439235\n",
      "  (0, 258)\t0.022800976228\n",
      "  (0, 321)\t0.135227444851\n",
      "  (0, 1902)\t0.0308464534379\n",
      "  (0, 60)\t0.0378614463125\n",
      "  (0, 1568)\t0.0410632747207\n",
      "  (0, 133)\t0.0149246554176\n",
      "  (0, 1035)\t0.0270514345596\n",
      "  (0, 676)\t0.0275049894885\n",
      "  (0, 1866)\t0.019526188197\n",
      "  (0, 1860)\t0.0390588267783\n",
      "  (0, 31)\t0.0327155550084\n",
      "  (0, 1518)\t0.032125271724\n",
      "  (0, 1037)\t0.0340148641308\n",
      "  (0, 1160)\t0.00874229915048\n",
      "  (0, 144)\t0.0268711363183\n",
      "  (0, 661)\t0.0131524238845\n",
      "  (0, 784)\t0.0725696389225\n",
      "  (0, 1268)\t0.265063690659\n",
      "  (0, 1596)\t0.0193696067692\n",
      "  (0, 1796)\t0.0221177268503\n",
      "  (0, 735)\t0.0148838694652\n",
      "  (0, 663)\t0.0412798937154\n",
      "  :\t:\n",
      "  (1599, 245)\t0.0334865323579\n",
      "  (1599, 1284)\t0.0216294445495\n",
      "  (1599, 686)\t0.0280888317761\n",
      "  (1599, 644)\t0.0446450507338\n",
      "  (1599, 460)\t0.0239579111362\n",
      "  (1599, 805)\t0.0463063564139\n",
      "  (1599, 221)\t0.0424863628227\n",
      "  (1599, 1112)\t0.0304637440019\n",
      "  (1599, 1361)\t0.034124104905\n",
      "  (1599, 1039)\t0.0240698390369\n",
      "  (1599, 1831)\t0.0302977920715\n",
      "  (1599, 1372)\t0.045716056436\n",
      "  (1599, 162)\t0.0345326767451\n",
      "  (1599, 628)\t0.0365946143793\n",
      "  (1599, 708)\t0.0424863628227\n",
      "  (1599, 420)\t0.0424863628227\n",
      "  (1599, 1792)\t0.0362405381079\n",
      "  (1599, 414)\t0.0476370097784\n",
      "  (1599, 1929)\t0.0414227312615\n",
      "  (1599, 1304)\t0.0492354330473\n",
      "  (1599, 254)\t0.0401948587364\n",
      "  (1599, 459)\t0.0399153421782\n",
      "  (1599, 822)\t0.0501736225744\n",
      "  (1599, 328)\t0.0524651266606\n",
      "  (1599, 546)\t0.053917391617\n"
     ]
    }
   ],
   "source": [
    "X_train_best = kbest.fit_transform(X_train_vect, y_train)\n",
    "print(X_train_best.shape)\n",
    "print(X_train_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()   # define the logistic regression\n",
    "logreg.fit(X_train_best, y_train);   # we fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_vect = vect.transform(X_test)   # this is an important step\n",
    "X_test_best = kbest.transform(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8075"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_best, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[157,  42],\n",
       "       [ 35, 166]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
