{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT NOW? \n",
    "# or \n",
    "# Data Science Topics I Didn't Cover and Where To Go From Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Example\n",
    "\n",
    "Let's say we want to extract some data from some api, like the google maps api. You will need several components for this process to work:\n",
    "\n",
    "1. A library that can handle and process requests to the api (we will use `requests`)\n",
    "2. A way to convert the data we get into a dictionary (since the json object itself is a string)\n",
    "3. A way to convert the json object into a dataframe\n",
    "\n",
    "Lets start by importing what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example I will show you involves an api that doesnt require any keys (most api's require some authentication, but the google maps one does not).\n",
    "\n",
    "This api requires that you pass in lat/long coordinates separated by commas and different locations separated by the | symbol (you can pass a whole sequence of locations up to some limit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = '42.974,-81.2053|42.9798,-81.1955'\n",
    "url = 'http://maps.googleapis.com/maps/api/elevation/json?locations='+locations+'&sensor=false'\n",
    "my_r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dude = None\n",
    "with open(\"/Users/sergeyfogelson/Downloads/this is my text\") as f:\n",
    "    dude = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'results': [{u'elevation': 242.08984375,\n",
       "   u'location': {u'lat': 42.974, u'lng': -81.2053},\n",
       "   u'resolution': 9.543951988220215},\n",
       "  {u'elevation': 252.1887664794922,\n",
       "   u'location': {u'lat': 42.9798, u'lng': -81.1955},\n",
       "   u'resolution': 9.543951988220215}],\n",
       " u'status': u'OK'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'status', u'results']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_r.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'elevation': 242.08984375,\n",
       "  u'location': {u'lat': 42.974, u'lng': -81.2053},\n",
       "  u'resolution': 9.543951988220215},\n",
       " {u'elevation': 252.1887664794922,\n",
       "  u'location': {u'lat': 42.9798, u'lng': -81.1955},\n",
       "  u'resolution': 9.543951988220215}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_r.json()[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>location.lng</th>\n",
       "      <th>resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242.089844</td>\n",
       "      <td>42.9740</td>\n",
       "      <td>-81.2053</td>\n",
       "      <td>9.543952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>252.188766</td>\n",
       "      <td>42.9798</td>\n",
       "      <td>-81.1955</td>\n",
       "      <td>9.543952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    elevation  location.lat  location.lng  resolution\n",
       "0  242.089844       42.9740      -81.2053    9.543952\n",
       "1  252.188766       42.9798      -81.1955    9.543952"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_normalize(my_r.json()[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Scraping Using BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a web scraping library that allows you to parse webpages and extract useful data from html (the format in which all webpages are displayed on the web).\n",
    "\n",
    "Let's use `BeautifulSoup` on an example website (the reddit front page):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we request the webpage and extract the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "a_website = requests.get(\"http://google.com\")\n",
    "data = BeautifulSoup(a_website.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what our data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE doctype html>\\n<html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en\"><head><meta content=\"Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\" name=\"description\"><meta content=\"noodp\" name=\"robots\"><meta content=\"text/html; charset=unicode-escape\" http-equiv=\"Content-Type\"><meta content=\"/logos/doodles/2016/2016-doodle-fruit-games-day-14-5645577527230464-hp.gif\" itemprop=\"image\"><meta content=\"Check out the 2016 Doodle Fruit Games at g.co/fruit #GoogleDoodle\" property=\"og:description\"><meta content=\"http://www.google.com/logos/doodles/2016/2016-doodle-fruit-games-day-14-5645577527230464-thp.png\" property=\"og:image\"><meta content=\"665\" property=\"og:image:width\"><meta content=\"220\" property=\"og:image:height\"><title>Google</title><script>(function(){window.google={kEI:'yT22V-61PMeHmQG7u7fwAw',kEXPI:'1351468,1351568,1351655,3700327,3700389,4029815,4031109,4032677,4036509,4036527,4038012,4039268,4040135,4043492,4045841,4048347,4052304,4061155,4062706,4063881,4063929,4064493,4065786,4065793,4065918,4066654,4066707,4067175,4067860,4068550,4068604,4069148,4069477,4069839,4069841,4069904,4070042,4070127,4070220,4070455,4070598,4071228,4071577,4071606,4072000,4072128,4072211,4072288,4072364,4072624,4072682,4072776,4073114,4073155,4073248,4073405,4073418,4073726,4073794,4073889,4073915,4073945,4073959,4073979,4074539,4074680,4074800,4074801,4074957,4075720,4075782,4075860,4076017,4076022,4076095,4076319,4076569,8300096,8300272,8502184,8503585,8504846,8504892,8504930,8505150,8505152,8505677,10200083,10200095,10201956,10201991,10202015,10202025,16200026,16200051,16200064',authuser:0,kscs:'c9c918f0_24'};google.kHL='en';})();(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||google.kEI};google.getLEI=function(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b};google.https=function(){return\"https:\"==window.location.protocol};google.ml=function(){return null};google.wl=function(a,b){try{google.ml(Error(a),!1,b)}catch(c){}};google.time=function(){return(new Date).getTime()};google.log=function(a,b,c,e,g){a=google.logUrl(a,b,c,e,g);if(\"\"!=a){b=new Image;var d=google.lc,f=google.li;d[f]=b;b.onerror=b.onload=b.onabort=function(){delete d[f]};window.google&&window.google.vel&&window.google.vel.lu&&window.google.vel.lu(a);b.src=a;google.li=f+1}};google.logUrl=function(a,b,c,e,g){var d=\"\",f=google.ls||\"\";if(!c&&-1==b.search(\"&ei=\")){var h=google.getEI(e),d=\"&ei=\"+h;-1==b.search(\"&lei=\")&&((e=google.getLEI(e))?d+=\"&lei=\"+e:h!=google.kEI&&(d+=\"&lei=\"+google.kEI))}a=c||\"/\"+(g||\"gen_204\")+\"?atyp=i&ct=\"+a+\"&cad=\"+b+d+f+\"&zx=\"+google.time();/^http:/i.test(a)&&google.https()&&(google.ml(Error(\"a\"),!1,{src:a,glmm:1}),a=\"\");return a};google.y={};google.x=function(a,b){google.y[a.id]=[a,b];return!1};google.load=function(a,b,c){google.x({id:a+k++},function(){google.load(a,b,c)})};var k=0;})();var _gjwl=location;function _gjuc(){var a=_gjwl.href.indexOf(\"#\");if(0<=a&&(a=_gjwl.href.substring(a),0<a.indexOf(\"&q=\")||0<=a.indexOf(\"#q=\"))&&(a=a.substring(1),-1==a.indexOf(\"#\"))){for(var d=0;d<a.length;){var b=d;\"&\"==a.charAt(b)&&++b;var c=a.indexOf(\"&\",b);-1==c&&(c=a.length);b=a.substring(b,c);if(0==b.indexOf(\"fp=\"))a=a.substring(0,d)+a.substring(c,a.length),c=d;else if(\"cad=h\"==b)return 0;d=c}_gjwl.href=\"/search?\"+a+\"&cad=h\";return 1}return 0}\\nfunction _gjh(){!_gjuc()&&window.google&&google.x&&google.x({id:\"GJH\"},function(){google.nav&&google.nav.gjh&&google.nav.gjh()})};window._gjh&&_gjh();</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\\n</style><style>body,td,a,p,.h{font-family:arial,sans-serif}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#36c}.q{color:#00c}.ts td{padding:0}.ts{border-collapse:collapse}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px}input{font-family:inherit}a.gb1,a.gb2,a.gb3,a.gb4{color:#11c !important}body{background:#fff;color:black}a{color:#11c;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#36c}a:visited{color:#551a8b}a.gb1,a.gb4{text-decoration:underline}a.gb3:hover{text-decoration:none}#ghead a.gb2:hover{color:#fff !important}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-left:13px;font-size:11px}.lsbb{background:#eee;border:solid 1px;border-color:#ccc #999 #999 #ccc;height:30px}.lsbb{display:block}.ftl,#fll a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#ccc}.lst:focus{outline:none}</style><script></script><link href=\"/images/branding/product/ico/googleg_lodp.ico\" rel=\"shortcut icon\"/></meta></meta></meta></meta></meta></meta></meta></meta></head><body bgcolor=\"#fff\"><script>(function(){var src='/images/nav_logo229.png';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\\n}\\n})();</script><div id=\"mngb\"> <div id=\"gbar\"><nobr><b class=\"gb1\">Search</b> <a class=\"gb1\" href=\"http://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a> <a class=\"gb1\" href=\"http://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a> <a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a> <a class=\"gb1\" href=\"http://www.youtube.com/?tab=w1\">YouTube</a> <a class=\"gb1\" href=\"http://news.google.com/nwshp?hl=en&amp;tab=wn\">News</a> <a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a> <a class=\"gb1\" href=\"https://www.google.com/intl/en/options/\" style=\"text-decoration:none\"><u>More</u> \\xbb</a></nobr></div><div id=\"guser\" width=\"100%\"><nobr><span class=\"gbi\" id=\"gbn\"></span><span class=\"gbf\" id=\"gbf\"></span><span id=\"gbe\"></span><a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a> | <a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a> | <a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=http://www.google.com/\" id=\"gb_70\" target=\"_top\">Sign in</a></nobr></div><div class=\"gbh\" style=\"left:0\"></div><div class=\"gbh\" style=\"right:0\"></div> </div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><a href=\"/search?site=&amp;ie=UTF-8&amp;q=Olympics&amp;oi=ddle&amp;ct=2016-doodle-fruit-games-day-14-5645577527230464-hp&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiuvs3CiMzOAhXHQyYKHbvdDT4QPQgD\"><img alt=\"Day 14 of the 2016 Doodle Fruit Games! Find out more at g.co/fruit\" border=\"0\" height=\"220\" id=\"hplogo\" onload=\"window.lol&amp;&amp;lol()\" src=\"/logos/doodles/2016/2016-doodle-fruit-games-day-14-5645577527230464-hp.gif\" title=\"Day 14 of the 2016 Doodle Fruit Games! Find out more at g.co/fruit\" width=\"665\"><br/></img></a><br/></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">\\xa0</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" type=\"hidden\" value=\"ISO-8859-1\"><input name=\"hl\" type=\"hidden\" value=\"en\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><input autocomplete=\"off\" class=\"lst\" maxlength=\"2048\" name=\"q\" size=\"57\" style=\"color:#000;margin:0;padding:5px 8px 0 6px;vertical-align:top\" title=\"Google Search\" value=\"\"/></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" name=\"btnG\" type=\"submit\" value=\"Google Search\"/></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" name=\"btnI\" onclick=\"if(this.form.q.value)this.checked=1; else top.location='/doodles/'\" type=\"submit\" value=\"I'm Feeling Lucky\"/></span></span></br></input></input></input></input></input></td><td align=\"left\" class=\"fl sblc\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a><a href=\"/language_tools?hl=en&amp;authuser=0\">Language tools</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"/></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br/></div><span id=\"footer\"><div style=\"font-size:10pt\"><div id=\"fll\" style=\"margin:19px auto;text-align:center\"><a href=\"/intl/en/ads/\">Advertising\\xa0Programs</a><a href=\"/services/\">Business Solutions</a><a href=\"https://plus.google.com/116899029375914044550\" rel=\"publisher\">+Google</a><a href=\"/intl/en/about.html\">About Google</a></div></div><p style=\"color:#767676;font-size:8pt\">\\xa9 2016 - <a href=\"/intl/en/policies/privacy/\">Privacy</a> - <a href=\"/intl/en/policies/terms/\">Terms</a></p></span></br></center><script>(function(){window.google.cdo={height:0,width:0};(function(){var a=window.innerWidth,b=window.innerHeight;if(!a||!b)var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body,a=d.clientWidth,b=d.clientHeight;a&&b&&(a!=google.cdo.width||b!=google.cdo.height)&&google.log(\"\",\"\",\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI);})();})();</script><div id=\"xjsd\"></div><div id=\"xjsi\"><script>(function(){function c(b){window.setTimeout(function(){var a=document.createElement(\"script\");a.src=b;document.getElementById(\"xjsd\").appendChild(a)},0)}google.dljp=function(b,a){google.xjsu=b;c(a)};google.dlj=c;})();(function(){window.google.xjsrm=[];})();if(google.y)google.y.first=[];if(!google.xjs){window._=window._||{};window._._DumpException=function(e){throw e};if(google.timers&&google.timers.load.t){google.timers.load.t.xjsls=new Date().getTime();}google.dljp('/xjs/_/js/k\\\\x3dxjs.hp.en_US.8q7yooQuBnE.O/m\\\\x3dsb_he,d/rt\\\\x3dj/d\\\\x3d1/t\\\\x3dzcms/rs\\\\x3dACT90oGEkQ2Aia2v-aoN4KGJyT0LpGxxow','/xjs/_/js/k\\\\x3dxjs.hp.en_US.8q7yooQuBnE.O/m\\\\x3dsb_he,d/rt\\\\x3dj/d\\\\x3d1/t\\\\x3dzcms/rs\\\\x3dACT90oGEkQ2Aia2v-aoN4KGJyT0LpGxxow');google.xjs=1;}google.pmc={\"sb_he\":{\"agen\":true,\"cgen\":true,\"client\":\"heirloom-hp\",\"dh\":true,\"dhqt\":true,\"ds\":\"\",\"fl\":true,\"host\":\"google.com\",\"isbh\":28,\"jam\":0,\"jsonp\":true,\"msgs\":{\"cibl\":\"Clear Search\",\"dym\":\"Did you mean:\",\"lcky\":\"I\\\\u0026#39;m Feeling Lucky\",\"lml\":\"Learn more\",\"oskt\":\"Input tools\",\"psrc\":\"This search was removed from your \\\\u003Ca href=\\\\\"/history\\\\\"\\\\u003EWeb History\\\\u003C/a\\\\u003E\",\"psrl\":\"Remove\",\"sbit\":\"Search by image\",\"srch\":\"Google Search\"},\"nds\":true,\"ovr\":{},\"pq\":\"\",\"refpd\":true,\"rfs\":[],\"scd\":10,\"sce\":5,\"stok\":\"nYDob0Bk21T7F_0kcirpW3FM0gQ\"},\"d\":{}};google.y.first.push(function(){if(google.med){google.med('init');google.initHistory();google.med('history');}});if(google.j&&google.j.en&&google.j.xi){window.setTimeout(google.j.xi,0);}\\n</script></div></body></html>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_list = data.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is the raw html from the website we just obtained. Now, what if we could extract specific things from that html, like all of the other webpages it links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.google.com/imghp?hl=en&tab=wi\n",
      "http://maps.google.com/maps?hl=en&tab=wl\n",
      "https://play.google.com/?hl=en&tab=w8\n",
      "http://www.youtube.com/?tab=w1\n",
      "http://news.google.com/nwshp?hl=en&tab=wn\n",
      "https://mail.google.com/mail/?tab=wm\n",
      "https://drive.google.com/?tab=wo\n",
      "https://www.google.com/intl/en/options/\n",
      "http://www.google.com/history/optout?hl=en\n",
      "/preferences?hl=en\n",
      "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
      "/search?site=&ie=UTF-8&q=Olympics&oi=ddle&ct=2016-doodle-fruit-games-day-14-5645577527230464-hp&hl=en&sa=X&ved=0ahUKEwiuvs3CiMzOAhXHQyYKHbvdDT4QPQgD\n",
      "/advanced_search?hl=en&authuser=0\n",
      "/language_tools?hl=en&authuser=0\n",
      "/intl/en/ads/\n",
      "/services/\n",
      "https://plus.google.com/116899029375914044550\n",
      "/intl/en/about.html\n",
      "/intl/en/policies/privacy/\n",
      "/intl/en/policies/terms/\n"
     ]
    }
   ],
   "source": [
    "for links in data.find_all('a'):\n",
    "    print (links.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"gb1\" href=\"http://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>,\n",
       " <a class=\"gb1\" href=\"http://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>,\n",
       " <a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>,\n",
       " <a class=\"gb1\" href=\"http://www.youtube.com/?tab=w1\">YouTube</a>,\n",
       " <a class=\"gb1\" href=\"http://news.google.com/nwshp?hl=en&amp;tab=wn\">News</a>,\n",
       " <a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>,\n",
       " <a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>,\n",
       " <a class=\"gb1\" href=\"https://www.google.com/intl/en/options/\" style=\"text-decoration:none\"><u>More</u> \\xbb</a>,\n",
       " <a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a>,\n",
       " <a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>,\n",
       " <a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=http://www.google.com/\" id=\"gb_70\" target=\"_top\">Sign in</a>,\n",
       " <a href=\"/search?site=&amp;ie=UTF-8&amp;q=Olympics&amp;oi=ddle&amp;ct=2016-doodle-fruit-games-day-13-5751002868219904-hp&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwit8ZyN5MnOAhVFHR4KHQqHClAQPQgD\"><img alt=\"Day 13 of the 2016 Doodle Fruit Games! Find out more at g.co/fruit\" border=\"0\" height=\"197\" id=\"hplogo\" onload=\"window.lol&amp;&amp;lol()\" src=\"/logos/doodles/2016/2016-doodle-fruit-games-day-13-5751002868219904-hp.gif\" title=\"Day 13 of the 2016 Doodle Fruit Games! Find out more at g.co/fruit\" width=\"387\"><br/></img></a>,\n",
       " <a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a>,\n",
       " <a href=\"/language_tools?hl=en&amp;authuser=0\">Language tools</a>,\n",
       " <a href=\"/intl/en/ads/\">Advertising\\xa0Programs</a>,\n",
       " <a href=\"/services/\">Business Solutions</a>,\n",
       " <a href=\"https://plus.google.com/116899029375914044550\" rel=\"publisher\">+Google</a>,\n",
       " <a href=\"/intl/en/about.html\">About Google</a>,\n",
       " <a href=\"/intl/en/policies/privacy/\">Privacy</a>,\n",
       " <a href=\"/intl/en/policies/terms/\">Terms</a>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.body.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Engines (or Recommendation Systems)\n",
    "\n",
    "![recommendation engine example](../images/recommendation_engine.png)\n",
    "\n",
    "### Problem: How can we recommend the \"most useful\" or \"most likely to be consumed and enjoyed\" items (books/movies/songs/media/clothing/investment products/etc.) to people?\n",
    "\n",
    "### 2 Basic Types of Recommendation Engines:\n",
    "\n",
    "### **Content-based:** Use the properties of items to recommend \"similar\" items (PANDORA DOES THIS)\n",
    "\n",
    "### **Collaborative:** Use the user's ratings on items and the ratings of others on those same items to recommend other items that \"similar\" users have also given high ratings (AMAZON/NETFLIX DO THIS)\n",
    "\n",
    "### Are there Python libraries for this?\n",
    "\n",
    "### [Turi](https://turi.com/index.html) has a really easy to use recommendation engine module, but its NOT FREE (BOO) AND WAS ALSO RECENTLY ACQUIRED BY APPLE\n",
    "### In general, recommenders are built at scale using other languages (Python has pretty terrible support for recommendation engines/systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Deep Learning\n",
    "\n",
    "#### [A good introduction](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "### What is it?\n",
    "\n",
    "### A really fancy supervised classification/regression method\n",
    "\n",
    "![artificial neural network example](../images/artificial_neural_network.png)\n",
    "\n",
    "### What is it used on? Datasets that are really really big that require learning really really complicated representations:\n",
    "\n",
    "### Pictures/Videos, Text:\n",
    "\n",
    "![facial recognition deep learning example](../images/calista-deepface.png)\n",
    "\n",
    "### What is it used for? Making object detection and labeling problems/classifying and rating text (sentiment analysis), low-level content \"understanding\"\n",
    "\n",
    "### Are there Python libraries for this?\n",
    "\n",
    "### [tensorflow](https://www.tensorflow.org) is google's new deep learning package. It just came out but everyone is going bonkers over it because GOOGLE.\n",
    "### [theano](http://deeplearning.net/software/theano/) this is much more established, works very well with both your CPU and your graphics card (whaaaaat??)\n",
    "### [keras](http://keras.io) is another Python library that you can use to build deep learning models and hook into either theano or tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### How can I use machine learning when I have freeform text instead of a matrix with rows and columns? How can I cluster texts? How can I try to model useful things from text generally?\n",
    "\n",
    "### Are there Python libraries for this?\n",
    "### [nltk](http://www.nltk.org/) is the grandaddy NLP library in Python, but its kinda fallen behind a bit\n",
    "### [spacy](https://spacy.io/) is a newer, more modern NLP library\n",
    "### [gensim](https://radimrehurek.com/gensim/index.html) is a very cool new library that allows you to learn word vector representations from text (so you can create text topics and other really cool stuff)\n",
    "\n",
    "You can even do some basic NLP transformations in Scikit-learn:\n",
    "\n",
    "We will go over 2 simple transformations to convert text into vectors:\n",
    "\n",
    "1. **CountVectorizer**: Really large-scale \"One-hot-encoding\" across all tokens in the text\n",
    "2. **TF-IDF**: Term frequency/Inverse Document Frequency - label documents according to the tokens that best distinguish it. Tokens that only appear in this document relative to the other documents in your dataset have high values, tokens that appear frequently across all documents get low values.\n",
    "\n",
    "**Both of these approaches completely ignore word order and context, simply convert text into a dictionary of values.**\n",
    "\n",
    "A token can be anything: a word, a character, a continuous group of 2,3,4 words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_text = pd.DataFrame([[\"yowza thats awesome\"],[\"Yowza yowza that is an awesome sauce\"]],columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yowza thats awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yowza yowza that is an awesome sauce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0                   yowza thats awesome\n",
       "1  Yowza yowza that is an awesome sauce"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "\n",
    "Think of it as large-scale one-hot encoding with some catches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,6))#stop_words=\"english\",stop_words=\"english\")\n",
    "transformed_text = vectorizer.fit_transform(some_text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>an awesome</th>\n",
       "      <th>an awesome sauce</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awesome sauce</th>\n",
       "      <th>is</th>\n",
       "      <th>is an</th>\n",
       "      <th>is an awesome</th>\n",
       "      <th>is an awesome sauce</th>\n",
       "      <th>sauce</th>\n",
       "      <th>...</th>\n",
       "      <th>yowza that is an</th>\n",
       "      <th>yowza that is an awesome</th>\n",
       "      <th>yowza that is an awesome sauce</th>\n",
       "      <th>yowza thats</th>\n",
       "      <th>yowza thats awesome</th>\n",
       "      <th>yowza yowza</th>\n",
       "      <th>yowza yowza that</th>\n",
       "      <th>yowza yowza that is</th>\n",
       "      <th>yowza yowza that is an</th>\n",
       "      <th>yowza yowza that is an awesome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  an awesome  an awesome sauce  awesome  awesome sauce  is  is an  \\\n",
       "0   0           0                 0        1              0   0      0   \n",
       "1   1           1                 1        1              1   1      1   \n",
       "\n",
       "   is an awesome  is an awesome sauce  sauce               ...                \\\n",
       "0              0                    0      0               ...                 \n",
       "1              1                    1      1               ...                 \n",
       "\n",
       "   yowza that is an  yowza that is an awesome  yowza that is an awesome sauce  \\\n",
       "0                 0                         0                               0   \n",
       "1                 1                         1                               1   \n",
       "\n",
       "   yowza thats  yowza thats awesome  yowza yowza  yowza yowza that  \\\n",
       "0            1                    1            0                 0   \n",
       "1            0                    0            1                 1   \n",
       "\n",
       "   yowza yowza that is  yowza yowza that is an  yowza yowza that is an awesome  \n",
       "0                    0                       0                               0  \n",
       "1                    1                       1                               1  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transformed_text.todense(),columns =vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'awesome', u'sauce', u'thats', u'yowza']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "- This computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- Much more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents). A term that appears across all documents is incredibly uninformative, a term that appears \n",
    "- Commonly used for search engine scoring, text summarization, document clustering until ~2013-2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>an awesome</th>\n",
       "      <th>an awesome sauce</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awesome sauce</th>\n",
       "      <th>is</th>\n",
       "      <th>is an</th>\n",
       "      <th>is an awesome</th>\n",
       "      <th>is an awesome sauce</th>\n",
       "      <th>sauce</th>\n",
       "      <th>...</th>\n",
       "      <th>yowza that is an</th>\n",
       "      <th>yowza that is an awesome</th>\n",
       "      <th>yowza that is an awesome sauce</th>\n",
       "      <th>yowza thats</th>\n",
       "      <th>yowza thats awesome</th>\n",
       "      <th>yowza yowza</th>\n",
       "      <th>yowza yowza that</th>\n",
       "      <th>yowza yowza that is</th>\n",
       "      <th>yowza yowza that is an</th>\n",
       "      <th>yowza yowza that is an awesome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.138134</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "      <td>0.194143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         an  an awesome  an awesome sauce   awesome  awesome sauce        is  \\\n",
       "0  0.000000    0.000000          0.000000  0.317800       0.000000  0.000000   \n",
       "1  0.194143    0.194143          0.194143  0.138134       0.194143  0.194143   \n",
       "\n",
       "      is an  is an awesome  is an awesome sauce     sauce  \\\n",
       "0  0.000000       0.000000             0.000000  0.000000   \n",
       "1  0.194143       0.194143             0.194143  0.194143   \n",
       "\n",
       "                ...                yowza that is an  yowza that is an awesome  \\\n",
       "0               ...                        0.000000                  0.000000   \n",
       "1               ...                        0.194143                  0.194143   \n",
       "\n",
       "   yowza that is an awesome sauce  yowza thats  yowza thats awesome  \\\n",
       "0                        0.000000     0.446656             0.446656   \n",
       "1                        0.194143     0.000000             0.000000   \n",
       "\n",
       "   yowza yowza  yowza yowza that  yowza yowza that is  yowza yowza that is an  \\\n",
       "0     0.000000          0.000000             0.000000                0.000000   \n",
       "1     0.194143          0.194143             0.194143                0.194143   \n",
       "\n",
       "   yowza yowza that is an awesome  \n",
       "0                        0.000000  \n",
       "1                        0.194143  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,6))#\n",
    "pd.DataFrame(vect.fit_transform(some_text.text).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIG DATA\n",
    "\n",
    "### Problem: Data Science is great and all on my little laptop, but can it scale to billions/trillions/septillions of examples?\n",
    "\n",
    "## YES! JUST ADD BIG DATA\n",
    "\n",
    "![big data](../images/big_data.jpg)\n",
    "\n",
    "### What this really means is, use api's that can scale across many many interconnected computers (servers)\n",
    "\n",
    "### Can you BIG DATA in Python?\n",
    "\n",
    "### [Spark](http://spark.apache.org) has a Python api. It's a big data framework (currently the hottest big data framework) that takes many of the algorithms and approaches we learned (data analysis, exploration, etc.) and scales them to work across many computers (DataFrames are implemented in Spark, so is Logistic Regression/Linear Regression and Random Forests)\n",
    "### [H20.ai](http://www.h2o.ai/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK WHAT DO I DO NEXT?\n",
    "\n",
    "### 1. Use what you've learned (code in Python when you want to use Excel)\n",
    "### 2. Go a bit deeper into the mathematics behind some of these algorithms (you need to know a little bit of 1st year calculus and a bit more linear algebra) by [reading one of the classic texts on the topic FOR FREE](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
    "### 3. Learn more! Check [DataTau](http://www.datatau.com) every day. lurk on the [machine learning](https://www.reddit.com/r/MachineLearning/) and [statistics](https://www.reddit.com/r/statistics) subreddits (yes there are useful things on reddit).\n",
    "### 4. Keep coding. As frequently as you can (or want).\n",
    "### 5. Keep learning. Check out [metacademy](http://www.metacademy.org/roadmaps/). They have self-directed, free course road maps to LEVEL UP.\n",
    "### 6. All of you are very capable of doing this. Keep going, because I personally think this stuff is really fun (and these skills will probably inform the future of every freaking industry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
