{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS 3 AGENDA\n",
    "* **Address questions/concerns from class 2**\n",
    "* **Introducing the pandas library**\n",
    "* **Working with the movielens movie ratings dataset**\n",
    "* **Combining datasets (next notebook)**\n",
    "* **Homework: NYC Taxi Data (2 notebooks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting use of pandas:\n",
    "\n",
    "https://github.com/datadesk/kobe-every-shot-ever/blob/master/kobe-every-shot-ever.ipynb\n",
    "\n",
    "### Repo for pandas practice:\n",
    "\n",
    "https://github.com/guipsamora/pandas_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is almost always the case when working with **Python**, we are going to need more than just its basic functionality available to us as we develop our analytical pipelines. \n",
    "\n",
    "In order to have this additional functionality available (being able to use **pandas**), we will rely on a  couple `import` statements.\n",
    "\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import print_function, unicode_literals, division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above did two things:\n",
    "\n",
    "* Loaded in all of the functionality that **pandas** provides (`import pandas as pd`)\n",
    "* Loaded in some additional functionality from a different package that **pandas** relies on called **NumPy** (`import numpy as np`)\n",
    "\n",
    "Importantly, `pd` is now the alias (new name) for the entire `pandas` library and `np` is the alias for the `numpy` library. Instead of having to type `pandas.something` or `numpy.something` to access a given function, you can now just type `pd` or `np`. \n",
    "\n",
    "So what exactly is [**pandas**](http://pandas.pydata.org) and why the funny name (we will talk about [**NumPy**](http://www.numpy.org) a bit later)?\n",
    "\n",
    "**pandas** is a Data Analysis Library written in and for the **Python** programming language and is a very loose acronym for **P**ython **An**alysis of **Da**taset**s** (or something like that anyway). \n",
    "\n",
    "It provides open source, easy-to-use data structures and data analysis tools and we will be relying on it heavily for the remainder of the course. Let's check the versions of `numpy` and `pandas` we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Pandas version:\",pd.__version__)\n",
    "print(\"Numpy version:\",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets begin by loading in our first dataset, the Movielens 1M dataset.\n",
    "\n",
    "This is a file that contains ~1,000,000 movie ratings taken from a website where users can rate a variety of movies on a 1-5 rating scale. Each rating is comprised of the following information:\n",
    "\n",
    "* A user id, represented by a number (an `int`) \n",
    "* A movie id, also an `int` \n",
    "* A rating, an `int` that should vary from 1-5 (inclusive) \n",
    "* A timestamp, recording when the rating was made in seconds since UNIX epoch time (this is standardly set to 12:00AM January 1, 1970), also an `int`\n",
    "\n",
    "The records in this file are arranged in the following order and format:\n",
    "\n",
    "`UserID::MovieID::Rating::Timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `read_csv` function in pandas and try to simply load the dataset **as is** into a variable (actually an object) called `ratingData` and see if that gets it into a usable format (it won't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData = pd.read_csv(\"../data/movieData/ratings.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the documentation for `read_csv`, you'll see that it is very large and provides for lots of different functionality. \n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "\n",
    "As a first pass, we just passed the path to the file in as a ```string``` to the `read_csv` function, without any other arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the first few rows and see what we get using the `head` function on our newly loaded dataset `ratingData`. \n",
    "\n",
    "The `head` function returns the first 5 rows by default of the DataFrame you call it on. You can change it to be a larger or smaller number by passing in a positive `integer` into `head` as an argument like so: `ratingData.head(100)`.\n",
    "\n",
    "The function `tail` does the exact same thing, except with the last records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well that looks terrible.\n",
    "Lets diagnose the problems we see and make it unterrible: \n",
    "\n",
    "1. Everything is in a single column (so we can't separately look at ratings, user ids,movie ids, or timestamps)!\n",
    "2. The first row is used as the name of the only column (we call this the **header**), which is no good, as the first record shouldn't be the header, but an actual record.\n",
    "3. The timestamp is in a format that doesn't really tell us anything useful about when the ratings occurred.\n",
    "\n",
    "So, lets use some of the additional functionality of `read_csv` to load this dataset in cleanly, and hopefully that will solve problems **1 and 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData = pd.read_csv(\"../data/movieData/ratings.dat\",sep = \"::\",\n",
    "                         names = ['UserID','MovieID','Rating','Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the `ParserWarning:` message (if you get one) as it doesn't affect what we are doing, and lets just take a look at the data now. I'll explain exactly what I did below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUCH BETTER!\n",
    "\n",
    "So, what did we just do?\n",
    "\n",
    "The function `read_csv` has lots of functionality, as I had mentioned (and as you saw when you pulled up its documentation). One of its options is called `sep`, and allows you to provide your own separator for dividing the columns that you have in your dataset. \n",
    "\n",
    "The default separator for `read_csv` is the comma (`,`), since `csv` stands for **c**omma **s**eparated **v**alues. \n",
    "\n",
    "However, since `::` separated the fields in this dataset, we supplied that as an argument (again, as a `string`) to the argument `sep` instead.\n",
    "\n",
    "A separate argument, `names`, allows you to pass in your own list of names, again as `strings`, to `read_csv` to be treated as the column names (or the **header**) of the dataset. Since we knew what the names for the columns should be, we put them in.\n",
    "\n",
    "Now that our dataset looks more reasonable, lets do a couple brief sanity checks and then fix issue **3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a sanity check and make sure that:\n",
    "\n",
    "1. All our data is in the format that we expect (everything is an `int`).\n",
    "2. The ratings range across the values we expect (1-5 and nothing else).\n",
    "\n",
    "The property `dtypes` is accessible from our `ratingData` object, and tells us the types of the data in all of our columns (which addresses **1.**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so everything appears to be an `int` as `int64` is an `integer` (positive or negative whole number) data type that can represent very large numbers.\n",
    "\n",
    "As an important aside about **pandas**, all the values in a given column have to be of the same type. So, **if even one value was not a whole number, (1.0 for example), the values for the entire column would be inferred to be something else (either a `float64` or an `object` if any of the entries were `strings`).**\n",
    "\n",
    "Now let's address **2.** by looking at all of the unique entries in the `Rating` column using `unique` (we expect there to be 5 unique values, 1-5 inclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as your column names do not contain strange characters (spaces and escape characters like !\\/), you can simply access the values in a column by doing `dataFrameName.columnName` where `dataFrameName` is the name of your `DataFrame` object and `columnName` is the name of your column.\n",
    "\n",
    "The function `unique` is accessible from our `ratingData` object, and simply returns all of the unique values within our column of choice as a `List`. You can also use `nunique` to simply return the number of distinct elements (as opposed to the elements themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Unique values:\",ratingData.Rating.unique())\n",
    "print(\"Number of unique values:\",ratingData.Rating.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try calling the .unique() function by using other column names in the dataset.\n",
    "# Data science is all about getting to know your data, so get to know the unique values of the columns in the dataset!\n",
    "#YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if your dataset contains weird column names, you have another way of accessing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData[\"Rating\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way of accessing the `Rating` column, we have to pass the name of the column as a `string` (in quotes \"\").\n",
    "\n",
    "What if we wanted to access multiple columns? Here's how you would do that (this is the only way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData[[\"Rating\",\"UserID\"]].head()\n",
    "# Is equivalent to:\n",
    "multipleColumns = [\"Rating\",\"UserID\"]\n",
    "ratingData[multipleColumns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multi-column case, you must pass the columns you are interested in as a `List` of `string` values (or as a `variable` that points to that list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on to fixing issue **3**.\n",
    "\n",
    "**pandas** has pretty fantastic date conversion functionality, as long as you know the format of the date data you are using. We know the format of our `Timestamp` column, so we are good to go.\n",
    "\n",
    "As a refresher, it was seconds since epoch time (12:00AM January 1, 1970).\n",
    "\n",
    "The pandas library has a function called `to_datetime` that, when given a column, and some optional parameters, converts the timestamp into nice, prettily formatted text.\n",
    "\n",
    "Lets create a new column called `FormattedTimestamp` by formatting our `Timestamp` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData['FormattedTimestamp'] = pd.to_datetime(ratingData.Timestamp,unit = 's')\n",
    "ratingData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there is a lot going on here, we are creating a new column `FormattedTimestamp` from a computation on another column, so lets work through it. \n",
    "\n",
    "To create a new column, you just use the same syntax as when you want to select a column, except you use a name that isn't found in the dataset's column list. \n",
    "\n",
    "Everything following the `=` sign in the expression is what you want to put into that new column.\n",
    "\n",
    "And what we did following the equals sign is:\n",
    "\n",
    "1. We passed the `Timestamp` column of our ratingData dataset as a mandatory parameter.\n",
    "2. We supplied an optional string parameter called `unit` with the unit of our data as a `string`.\n",
    "\n",
    "(Look at the documentation, and you can see there is lots more stuff you can do with `to_datetime`!).\n",
    "\n",
    "Now, we can simply remove the old `Timestamp` column, since we don't need it anymore!\n",
    "\n",
    "To do so, you simply pass the name of the dataframe and the column you are trying to delete to the `del` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Columns before removal: \", ratingData.columns)\n",
    "del ratingData[\"Timestamp\"]\n",
    "print(\"Columns after removal: \", ratingData.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `del` operation simply deletes the columns you specify from the given `DataFrame`. Remember that once you've deleted a given column, you can't delete it again, so executing the above cell muliple times will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing about dates before we move on, once you've got them converted to the pretty format we saw above, you can access all kinds of information from each date by calling the `dt` module from within the column that stores your dates.\n",
    "\n",
    "So, as an example, getting the year of every row in our dataset is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.FormattedTimestamp.dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other properties (like day, hour, day of month, etc.) are available as well.\n",
    "\n",
    "Again, just check the [timestamp documentation.](http://pandas.pydata.org/pandas-docs/stable/timeseries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try using the other functions found in the dt module: \n",
    "#call ratingData.FormattedTimestamp.dt and the use the Tab key\n",
    "#to see what else is offered\n",
    "\n",
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note about the outputs of calling these datetime **properties.** When you call the property on a single value (like a single row) or on an entire column you will get a `Series` object returned to you, which is the **pandas** representation of a single column.\n",
    "\n",
    "`Series` objects are different from `DataFrame` objects (which we've been working with exclusively thus far) because they can be appended (attached) as new columns to your dataset (which is already a `DataFrame` object) without any problems.\n",
    "\n",
    "So if you want to store the `year` of every row in a new column called `year` then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData[\"year\"] = ratingData.FormattedTimestamp.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make another column called \"month\" and call the appropriate function to store the month of each rating\n",
    "#do the same thing for \"day\", just to get a good handle on the kinds of things you can do\n",
    "#YOUR CODE HERE\n",
    "\n",
    "ratingData[\"month\"] = ratingData.FormattedTimestamp.dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on and get a better feel for our dataset now.\n",
    "\n",
    "What if we wanted to know the exact shape of our dataset? That is, the *exact* number of rows and columns found in it? (I told you there were ~1,000,000 ratings here, but exactly how many are there?\n",
    "\n",
    "We can use another **property** that is available in our `ratingData` object called `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(numRows,numColumns) = ratingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(numRows)\n",
    "print(numColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` property (which doesn't have any documentation, unfortunately), tells us the number of dimensions and the number of values in each dimension in our dataset (we can have 1, 2, or more dimensions in our dataset, after all), as a tuple (a very common datatype in **Python** that is of a fixed size and cannot be changed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a brief aside, you saw that I just said **property** and not **function** right? \n",
    "\n",
    "That means, this value is available to us as part of the dataset intrinsically and cannot be changed based on inputs (in CS parlance, it is read-only), and doesn't have to be called like `unique(), head(), tail(), max(), mean()` and all the other functions that are re-computed on the dataset because they can effectively change! \n",
    "\n",
    "(Another example property that is available to you and that you've already used is `dtypes`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, back to `shape`.\n",
    "\n",
    "The number of dimensions in our dataset is the arity of the tuple (the number of commas + 1) and the numbers between commas tell us the number of values in that dimension. So the **arity is 2 because all we have are rows and columns.** \n",
    "\n",
    "So, we have **1,000,209 distinct values in the first dimension** (here they are our ratings, or **rows**) and **4 distinct values in the second dimension** (here the second dimension is our **columns**, and is as we expect, since we just added a new column and deleted an old one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a bit of **pandas** functionality at your disposal, you should give me the answer to the following questions by writing a bit of code:\n",
    "\n",
    "1. How many users are there in the dataset?\n",
    "2. How many movies are in the dataset?\n",
    "3. How many unique times are in the dataset?\n",
    "\n",
    "**Hint: All you need is the `unique()` function and the `shape` property to answer these questions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE, replace \"pass\" with your code in all 3 cases\n",
    "## Write a print function to display your results.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! You've written your first bit of code using **pandas** and have actually answered some useful questions about this dataset! Pats on the back all around, and lets keep exploring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the ratings in the dataset to get a feel for their central values and spread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The average rating in the dataset is:\",ratingData.Rating.mean())\n",
    "print(\"The middle rating in the dataset is:\",ratingData.Rating.median())\n",
    "print(\"The standard deviation of the ratings is:\",ratingData.Rating.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the people in our dataset don't like to rate movies too low, as the movies have  an average rating >3 (which is supposed to be average on a 1-5 scale).\n",
    "\n",
    "The functions `mean`,`median`, and `std` compute exactly what you expect (the mean, median, and standard deviation {or spread} of a given set of values) and can even be applied to the entire dataset (although in our case, that doesnt make too much sense as all the other columns, except `Timestamp` are numeric mappings of categorical data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other descriptive stats can be computed as well (min/max, sum, variance, skew, kurtosis, etc.). Just take a look at the [descriptive statistics](http://pandas.pydata.org/pandas-docs/stable/basics.html#descriptive-statistics) documentation!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Look at the descriptive statistics documentation and print a couple more descriptive stats\n",
    "#on just the \"Rating\" column. You get to choose what stats you want to print\n",
    "#YOUR CODE HERE\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for a little trick! \n",
    "\n",
    "If you want all these basic statistics and a few more, just use the `describe` function, which works just like the others mentioned above (you can call it on just a single column, or the whole dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets learn how to subselect values within dataframes.\n",
    "\n",
    "Subselection in **pandas** works along the following very common pattern:\n",
    "\n",
    "1. You create a condition that can be evaluated to either **true** or **false** for every row in the dataset and store the outcome in a variable (this is traditionally called a **mask**).\n",
    "2. You apply that **mask** onto your `DataFrame` (dataset).\n",
    "\n",
    "Let's try this subselection + application with the ratings in our dataset by only getting all of the really low ratings (lets say low ratings are those that are < 3) in our dataset.\n",
    "\n",
    "We will create a mask that expresses our \"crappy ratings\" condition, and then apply that mask to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crappyRatingMask = ratingData.Rating < 3\n",
    "crappyRatings = ratingData[crappyRatingMask]\n",
    "crappyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(crappyRatings.Rating.unique())\n",
    "# This is a sanity check to make sure that our filtered data contains only low ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, this worked as expected!\n",
    "How many of these crappy ratings are there? (Replace **pass** in the code cell below with your answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for you to show how much you've learned. Give me the answers to the following questions:\n",
    "\n",
    "1. What was the average rating in January?\n",
    "* What was the average crappy rating in January?\n",
    "\n",
    "**Hint:** Although there are lots of ways you can tackle these questions (many of which you will learn soon). I suggest you use the following procedure:\n",
    "\n",
    "1. Create a new column in `ratingData` and call it \"month\", so that it tells you the month of every row\n",
    "* Create a new dataset of:\n",
    "  1. all January records and store it in `januaryRatings`\n",
    "  * create a mask of crappy ratings using that dataset called `crappyJanuaryRatingsMask`\n",
    "  * apply that `crappyJanuaryRatingsMask` to your dataset and store it in a new dataset called `crappyJanuaryRatings`\n",
    "  * compute the mean of the `Rating` column for both `januaryRatings` and `crappyJanuaryRatings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that you know how to do some basic subselection, sorting, and calculations on data, we are going to do something a bit more complicated, and start subdividing our data into groups to be able to answer some more general questions about our dataset.\n",
    "\n",
    "Once you have this functionality down, you will be able to: \n",
    "\n",
    "1. Answer more interesting kinds of questions\n",
    "* Answer the questions above using fewer lines of code \n",
    "\n",
    "Lets say you wanted to know or do the following:\n",
    "\n",
    "1. **In what month did users rate the most movies?**\n",
    "2. **What month had the highest average rating?**\n",
    "3. **Remove users with too few ratings (lets say < 30) and reanswer these same questions**\n",
    "\n",
    "Our approach here will be:\n",
    "\n",
    "* Learn to use the `groupby` functionality of **pandas** to create subgroups of our ratings based on either the `month` the ratings were given or on the `UserID` of the rater.\n",
    "* Apply an aggregating function to these groups to return:\n",
    "  * The `size` of each group (since the `size` of each group is either the number of ratings in that `month` or the number of ratings for that `UserID`)\n",
    "  * The `mean` of the `Rating` column within each group\n",
    "  * A filtered version of the original dataset so that only the groups that are large enough (when grouping on `UserID`, those users that have made enough ratings) are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that you know how to do some basic subselection, sorting, and calculations on data, we are going to do something a bit more complicated, and start subdividing our data into groups to be able to answer some more general questions about our dataset.\n",
    "\n",
    "Once you have this functionality down, you will be able to: \n",
    "\n",
    "1. Answer more interesting kinds of questions\n",
    "* Answer the questions above using fewer lines of code \n",
    "\n",
    "Lets say you wanted to know or do the following:\n",
    "\n",
    "1. **In what month did users rate the most movies?**\n",
    "2. **What month had the highest average rating?**\n",
    "3. **Remove users with too few ratings (lets say < 30) and reanswer these same questions.**\n",
    "\n",
    "Our approach here will be:\n",
    "\n",
    "* Learn to use the `groupby` functionality of **pandas** to create subgroups of our ratings based on either the `month` the ratings were given or on the `UserID` of the rater.\n",
    "* Apply an aggregating function to these groups to return:\n",
    "  * The `size` of each group (since the `size` of each group is either the number of ratings in that `month` or the number of ratings for that `UserID`)\n",
    "  * The `mean` of the `Rating` column within each group\n",
    "  * A filtered version of the original dataset so that only the groups that are large enough (when grouping on `UserID`, those users that have made enough ratings) are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` function in **pandas** is analagous to the grouping operations you may be familiar with if you've ever used any **SQL** variants. \n",
    "\n",
    "A generic **SQL** translation of **1.**, for example, would look something like:\n",
    "\n",
    "```\n",
    "SELECT month,numRatings FROM (SELECT month,size(month) AS numRatings\n",
    "FROM ratingData\n",
    "GROUP BY month) WHERE numRatings = MAX(numRatings)\n",
    "```\n",
    "\n",
    "(If this looks like wizardry, don't worry, I'm just trying to show this to those users that are familiar with SQL and any of its variants; this is the only SQL you will see all weekend!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping can get very complicated, but as a first pass you can think of it as a way to split your dataset into non-overlapping subsets along any axis (along rows or columns, in our case). \n",
    "\n",
    "The values along which you **group** your dataset are traditionally called **keys**, so **each key should be unique to each group, and each group can have at most one key associated with it** (although the key for identifying each group can be really complicated).\n",
    "\n",
    "Once you've grouped your dataset, the `GroupBy` object isn't too useful by itself. It becomes useful when you apply a **transformation** to it and get a new dataset back. \n",
    "\n",
    "We typically call this **transformation** an **aggregation**, as we are getting some aggregate value back for each group.\n",
    "\n",
    "The **aggregation** functions we will be using for questions **1.,2.** are `size` and `mean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, enough explaining, lets get to some hacking.\n",
    "\n",
    "Lets address grouping in the context of answering our first question:\n",
    "\n",
    "* **In what month did users rate the most movies?**\n",
    "\n",
    "In **pandas**, to create groups, you must create a `GroupBy` object (more on what that is later) from your `DataFrame` object (your dataset) by passing the values along which you want to group to the `groupby` function.\n",
    "\n",
    "We want to **groupby** the **month** column and store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthGroups = ratingData.groupby(\"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object, called a `DataFrameGroupBy` is a rearranging of the rows in the dataset into distinct collections, called groups. Let's take a look at the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(monthGroups.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't print `monthGroups` itself, as its not a `DataFrame`, but a rearranged representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(monthGroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we want to get the number of records (rows) in each group in our `monthGroups` object using the `size` function (which is accessible from this object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingsPerMonth = monthGroups.size()\n",
    "ratingsPerMonth #this will simply print the result inside of this notebook so we can see it\n",
    "#to print to the screen outside of the notebook, you would have to say \"print(ratingsPerMonth)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to make it nice and clean, we will simply output the month with the largest number of ratings using the `max` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthWithMostRatings = ratingsPerMonth[ratingsPerMonth==ratingsPerMonth.max()]\n",
    "monthWithMostRatings #same as above, print to notebook without having to say \"print highestRating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got it? Good!\n",
    "\n",
    "One more thing before you try it yourself. Once you've created a `groupby` object, you don't have to select everything in the object to perform an aggregation, and can subselect a given column within each group (**Hint, Hint!**)\n",
    "\n",
    "Now you try answering question **2.**:\n",
    "\n",
    "* **What was the average rating given to movies in each month of the year?**\n",
    "\n",
    "Create the objects `avgMonthlyRatings` and `highestAvgRating`, that allow you to answer the question and print them to the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These aggregations are already implemented in the `GroupBy` object and can be called directly from the object, but this is not generally the way this is handled.\n",
    "\n",
    "Under the hood, **pandas** is passing the function we apply (`mean` or `size` in our case) to another function called `aggregate` and that function is actually doing the heavy lifting.\n",
    "\n",
    "**The `aggregate` function operates on the entire group within the groupby, so any group-based operations must use an aggregating function.**\n",
    "\n",
    "So, what is actually happening when we wrote our code to answer question **1.** above like this:\n",
    "\n",
    "`ratingsPerMonth = monthGroups.size()`\n",
    "\n",
    "Was actually being implemented more like this:\n",
    "\n",
    "``ratingsPerMonth = monthGroups.aggregate(size)``\n",
    "\n",
    "or this:\n",
    "\n",
    "``ratingsPerMonth = monthGroups.agg(size)``\n",
    "\n",
    "So, this means that whenever we want to do more complicated transformations or reductions of our data, on a per-group level, we should supply our function(s) of interest to `aggregate` or the shorhand `agg` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple more explanations before we tackle filtering for answering question **3.**\n",
    "\n",
    "You can pass multiple functions to `aggregate` as a `List`, if you want multiple transformations applied to the data, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthGroups.Rating.agg([np.mean,np.size,np.std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these cases, you will notice that we have to call the functions using the **NumPy** module (but using the `np` alias), which was installed when you got **pandas** working on your system (since **NumPy** is a dependency of **pandas** and gives **pandas** all of the math and matrix wrangling functionality it relies on behind the scenes).\n",
    "\n",
    "**NumPy** is a really powerful matrix and math library in **Python** and has lots of functionality we won't go into here, so if you're interested, head over to their [website](http://www.numpy.org) to learn more!\n",
    "\n",
    "Now, lets move on to filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go through how to answer question **3.**:\n",
    "\n",
    "* **Remove users with too few ratings (lets say < 30) and reanswer these same questions**\n",
    "\n",
    "Here is our pipeline:\n",
    "\n",
    "1. `groupby ratingData on \"UserId\"` and call this new `GroupBy` object `userGroups`\n",
    "* `filter` so that only groups (users) containing > 30 ratings are kept in a new `DataFrame`, called `filteredRatings`\n",
    "* `groupby filteredRatings on \"month\"` and call this new `GroupBy` object `filteredMonthGroups`\n",
    "* recompute `mean` and `size` statistics on the `Rating` column in `filteredMonthGroups` and store it in a variable called `filteredMonthAggs`\n",
    "* use `max` to see if the month when the largest rating mean and rating size have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userGroups = ratingData.groupby(\"UserID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step is very similar to what we did before, except we are grouping on a different column, `UserID`.\n",
    "\n",
    "Now comes the more challenging part, using `filter`, and involves learning a bit about **anonymous functions**. Here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def enoughRatings(currGroup):\n",
    "    return currGroup.Rating.size >= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredRatings = userGroups.filter(lambda currGroup: currGroup.Rating.size >= 30)\n",
    "filteredRatings2 = userGroups.filter(enoughRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(filteredRatings.shape)\n",
    "print(filteredRatings2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AAAAA WHAT IS THAT? lambda? x? What are these things?**\n",
    "\n",
    "Ok, lets take a breath and work through this...\n",
    "\n",
    "The `filter` function takes a function as an input and requires that the function you pass to it return either `True` or `False` on a per-group basis. \n",
    "\n",
    "It then returns a new `DataFrame` object, sorted into the groups you grouped on initially, with all of the groups removed that don't satisfy the constraints of the function you passed to it (that is, removing those groups that, when the function is applied to them, return `False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why are we using this weird `lambda` thing? \n",
    "\n",
    "Well, `lambda` is the keyword for creating an **anonymous function** in **Python**.\n",
    "\n",
    "**anonymous functions** are functions that you define within some restricted place that:\n",
    "\n",
    "1. Usually accomplish some very minimal functionality\n",
    "2. Don't need the syntactic sugar that functions usually come with because of 1. and to maintain the compactness (and hopefully clarity) of your code.\n",
    "\n",
    "To declare an anonymous function you:\n",
    "\n",
    "1. type `lambda`, followed by\n",
    "* arbitrary names for the parameters that function accepts (x,y, etc.) separated by commas, followed by\n",
    "* a colon, which tells **Python** that we are done with specifying the parameters, followed by\n",
    "* the expression that defines how the function operates on the parameters. This expression will dictate what the function returns.\n",
    "\n",
    "Here's a really dumb anonymous function, stored in the variable `dumb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dumb = lambda x: x + 1\n",
    "print(dumb(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "`lambda x: x.Rating.size >= 30` means:\n",
    "\n",
    "1. **This function accepts a single parameter (in our case the group) arbitrarily called x.**\n",
    "* **It operates on this parameter, x, by finding some attribute in it called \"Rating\" (which it definitely has from earlier steps) and checking whether its size property is greater than or equal to 30.**\n",
    "* **Because this function is checking a condition, it will return either `True` or `False` for every parameter you pass to it**\n",
    "\n",
    "If you understand that, you grok **anonymous functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, if you don't understand WTF just happened, you can actually write your own function, and just pass it directly to the filter!**\n",
    "\n",
    "So, instead of:\n",
    "\n",
    "`filteredRatings = userGroups.filter(lambda x: x.Rating.size >= 30)`\n",
    "\n",
    "You can do this (they are functionally equivalent!):\n",
    "```\n",
    "def filterFewestRatings(dataset):\n",
    "    return dataset.Rating.size >= 30\n",
    "    \n",
    "filteredRatings = userGroups.filter(filterFewestRatings)\n",
    "```\n",
    "\n",
    "Here, we've replaced the variable `x` in the anonymous function with a more easily understood variable `dataset` in the named (non-anonymous) function `filterFewestRatings`. This has the same exact functionality as the **anonymous function**, but is clearly more code to write. \n",
    "\n",
    "**It's your choice as to how you want to implement small functions like this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at filtered ratings and see how many ratings and users we eliminated before we recompute our answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Original number of ratings:\",ratingData.shape[0])\n",
    "print(\"Filtered number of ratings:\",filteredRatings.shape[0])\n",
    "print(\"Fraction of ratings eliminated:\",(1.0-float(filteredRatings.shape[0])/ratingData.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Original number of users:\",ratingData.UserID.unique().size)\n",
    "print(\"Filtered number of users:\",filteredRatings.UserID.unique().size)\n",
    "print(\"Fraction of users eliminated:\",(1.0-float(filteredRatings.UserID.unique().size)/ratingData.UserID.unique().size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got rid of about 2% of the ratings, but over 12% of the users!\n",
    "\n",
    "Now that we've filtered, you should be able to do the rest and recompute the answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredMonthGroups = filteredRatings.groupby(\"month\")\n",
    "filteredMonthAggs = filteredMonthGroups.Rating.agg([np.mean,np.median,np.std,np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredMonthAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To hammer all of this home, try to answer the following:\n",
    "\n",
    "**What were the ID(s) of the average highest-rated movie in the first half of the year for those movies that were rated at least 5 times within that timespan?**\n",
    "\n",
    "Fill in the next cell by replacing the `pass` (or make more cells if you need to) and tell me when you know the answer (but quietly, so you don't tell everyone else!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple more data analysis functions before I set you loose on a completely new dataset!\n",
    "\n",
    "**pandas** provides some pretty cool basic statistical functionality apart from the really simple functions we've used so far (like `mean`, `std`, `max`, and `size`).\n",
    "\n",
    "What I'm talking about are statistical functions that compare two variables, specifically  **covariance** and **correlation**. \n",
    "\n",
    "Briefly, **covariance** is a way to tell whether two variables tend to move in the same or opposite directions, but cannot measure the absolute strength of this relationship (if two variables show positive covariance, it means that as one increases, so does the other one, and vice versa). \n",
    "\n",
    "**Correlation** on the other hand, can tell you both whether two variables tend to move with or against each other, and how strong this relationship between them is.\n",
    "\n",
    "I will be showing you a fairly contrived example here, but you will be using this functionality in the actual assignment you'll be working on.\n",
    "\n",
    "I will keep this short and sweet, because I know you are dying to get started on your own!\n",
    "\n",
    "To correlate two columns using the standard pearson correlation in **pandas** simply do something like this, where we correlate `month` with `Rating`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.month.corr(ratingData.Rating,method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get the full correlation matrix among all variables that are of a numeric type, just call `corr` on the whole `DataFrame`. **pandas** will know to only make the correlations among numeric columns only and will exclude all non-numeric columns from the resulting correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingData.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this is a contrived example because **the only truly numeric column here is Rating** all the other columns (except for `FormattedTimestamp`) are numeric labels for categorical items. \n",
    "\n",
    "What this means is that there is no way we can actually say that one `UserID` is greater than some other `UserID` because the mapping from the actual user's identity to the `UserID` number associated with them is arbitrary, and the same thing applies to every other numeric column here except for `Rating`. \n",
    "\n",
    "And because we can't rank any of the entries in the other columns in any meaningful way, **the correlation between them is meaningless.** \n",
    "\n",
    "I simply wanted to show you how you would do this, given the right situation (which you will experience shortly!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To consolidate what you've learned answer the following questions about this dataset:\n",
    "\n",
    "1. Which user rated the most movies?\n",
    "* Which movie id got the highest average rating?\n",
    "* Which movie id had the most varied rating?\n",
    "  * Of those movies that have been rated at least 5 times?\n",
    "* In which month were raters the most generous on average?\n",
    "* Which month had the most variation in ratings?\n",
    "* Which user had the most variation in ratings?\n",
    "* On what day of the week did users rate the most movies?\n",
    "* Which hour in the day did users rate the most movies?\n",
    "* What hour in the day had the highest average movie rating?\n",
    "* Which user gave movies the highest average rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
