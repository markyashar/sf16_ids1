{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Agenda\n",
    "\n",
    "  * Attempt to use linear regression for classification\n",
    "  * Logistic regression is a better alternative for classification\n",
    "  * Brief overview of probability, odds, e, log, and log-odds\n",
    "  * What is the logistic regression model?\n",
    "  * Interpreting logistic regression coefficients\n",
    "  * Compare logistic regression with other models\n",
    "  \n",
    "By the end of this portion of the class you will be able to:\n",
    "  * Use logistic regression for a classification problem in the future\n",
    "  * interpret the coefficients of a trained logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting a categorical response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the first part of today's lesson, we were attempting to predict a **continuous response**. However, what we want to do now is see if we can apply the same sort of logic to predict an outcome that has only 2 distinct possibilities, or what is known as a **categorical response.**\n",
    "\n",
    "In machine learning parlance, we looked at **regression** when we were using linear regression, but we are now going to try to use the same approach for what is known as a **classification** problem (problems with only a discrete, finite number of outcomes; in our case, just 2).\n",
    "\n",
    "As always, we are going to import all of the functionality we need before we get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Python 2 and 3 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "#data handling/modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to import a slightly different dataset. This dataset is also from the famed [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html) and can be found [here](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column).\n",
    "\n",
    "This dataset contains 6 biomechanical features used to classify orthopaedic patients into 2 classes - normal and abnormal:\n",
    "  * pelvic incidence\n",
    "  * pelvic tilt\n",
    "  * lumbar lordosis angle\n",
    "  * sacral slope\n",
    "  * pelvic radius\n",
    "  * grade of spondylolisthesis\n",
    "  \n",
    "Lets load the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertebral_data = pd.read_csv(\"../data/vertebral_column_2_categories.dat\", sep=\" \",\n",
    "                             names=[\"pelvic_incidence\",\"pelvic_tilt\",\"lumbar_lordosis_angle\",\"sacral_slope\",\"pelvic_radius\",\"spondy_grade\",\"outcome\"])\n",
    "vertebral_data.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use linear regression for this task, we have to convert our **categorical** target into a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertebral_data[\"outcome_number\"] = (vertebral_data.outcome=='AB').astype(int)\n",
    "vertebral_data.outcome_number.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so now our outcome is no longer a value, but a number. Let's plot `pelvic_incidence` relative to this new numeric `outcome_number`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(vertebral_data,x_vars=[\"pelvic_incidence\"],y_vars=\"outcome_number\", size=6, aspect=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And now lets do a simple linear regression on that feature like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions\n",
    "feature_cols = ['pelvic_incidence']\n",
    "X = vertebral_data[feature_cols]\n",
    "y = vertebral_data.outcome_number\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "outcome_pred = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets examine the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **pelvic_incidence=35**, what class do we predict for outcome? **0**\n",
    "\n",
    "So, we predict the 0 class for **lower** values of `pelvic_incidence`, and the 1 class for **higher** values of `pelvic_incidence`. What's our cutoff value? Around **pelvic_incidence=45**, because that's where the linear regression line crosses the midpoint (0.5) between predicting class 0 and class 1.\n",
    "\n",
    "So, we'll say that if **outcome_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transform predictions to 1 or 0\n",
    "outcome_pred_class = np.where(outcome_pred >= 0.5, 1, 0)\n",
    "outcome_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred_class, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What went wrong? This is a line plot, and it connects points in the order they are found. Let's sort the DataFrame by \"al\" to fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add predicted class to DataFrame\n",
    "vertebral_data['outcome_pred_class'] = outcome_pred_class\n",
    "\n",
    "# sort DataFrame by pelvic_incidence so that the line plot makes sense\n",
    "vertebral_data.sort('pelvic_incidence', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot the class predictions again\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, vertebral_data.outcome_pred_class, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[**Linear regression:**](https://en.wikipedia.org/wiki/Linear_regression) continuous response is modeled as a linear combination of the features used :\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x + ... \\beta_nx$$\n",
    "\n",
    "[**Logistic regression:**](https://en.wikipedia.org/wiki/Logistic_regression) model based on the [**logistic function**](https://en.wikipedia.org/wiki/Logistic_function) which takes any number and outputs a number between 0 and 1. We can interpret the output as a probability.\n",
    "\n",
    "**Logistic function:\n",
    "**\n",
    "$$y = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Here's what that looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logistic curve](../images/logistic_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input variable is just the linear combination of features: $$\\beta_0 + \\beta_1x + ... \\beta_nx$$\n",
    "\n",
    "The sklearn logistic regression function will choose the best coefficients to fit the data. We can then use the logistic function to calculate probabilties.\n",
    "\n",
    "$$P(y) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$$\n",
    "\n",
    "We can rearrange this equation:\n",
    "\n",
    "$$\\log \\left( \\frac{P(y)}{1-P(y)} \\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$$\n",
    "\n",
    "The thing on the left is called the log-odds (because it's the log of the odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words:\n",
    "\n",
    "- Logistic regression outputs the **probabilities of a specific class**\n",
    "- Those probabilities can be converted into **class predictions**:\n",
    "\n",
    "$f(x)= \n",
    "\\begin{cases}\n",
    "    1,& \\text{if } p\\geq 0.5\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The **logistic function** has some nice properties:\n",
    "\n",
    "- Takes on an \"s\" shape (which allows it to be differentiable, a really important math property for functions to have)\n",
    "- Output is bounded by 0 and 1\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "- **Multinomial logistic regression** is used when there are more than 2 classes.\n",
    "- Coefficients are estimated using **maximum likelihood estimation**, meaning that we choose parameters that maximize the likelihood of the observed data. We do this using fancy math involving taking derivatives, and thats why that S-shaped curve is so important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use Logistic Regression Instead of Linear Regression on Categorical Outcome Variables\n",
    "\n",
    "Logistic regression can do exactly what we just did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e9)\n",
    "feature_cols = ['pelvic_incidence']\n",
    "X = vertebral_data[feature_cols]\n",
    "y = vertebral_data.outcome_number\n",
    "logreg.fit(X, y)\n",
    "outcome_pred_class_log = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print the class predictions\n",
    "outcome_pred_class_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_pred_class_log, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we wanted the **predicted probabilities** instead of just the **class predictions**, to understand how confident we are in a given prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "outcome_probs = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities, and the 50% line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_probs, color='red')\n",
    "plt.plot(vertebral_data.pelvic_incidence,np.ones(outcome_probs.shape)*.5,'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "print(\"Pelvic incidence of 15:\", logreg.predict_proba(15))\n",
    "print(\"Pelvic incidence of 10:\", logreg.predict_proba(10))\n",
    "print(\"Pelvic incidence of 55:\", logreg.predict_proba(55))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are these numbers? \n",
    "\n",
    "The first number in each entry indicates the predicted probability of **class 0**, and the second number in each entry indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities again\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(vertebral_data.pelvic_incidence, vertebral_data.outcome_number)\n",
    "plt.plot(vertebral_data.pelvic_incidence, outcome_probs, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=2 using the predict_proba method\n",
    "logreg.predict_proba(55)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "zip(feature_cols, logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Interpretation:** A 1 unit increase in `pelvic_incidence` is associated with a ~0.054 unit increase in the log-odds of `outcome`, where a positive outcome is having a vertebral abnormality (not positive in the real world, but positive in how we coded our outcome feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=3 using the predict_proba method\n",
    "logreg.predict_proba(56)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What does this mean actually? \n",
    "\n",
    "**Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# examine the intercept\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Interpretation:** For a 'pelvic_incidence' value of 0, the log-odds of 'outcome' is -2.36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# convert log-odds to probability\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That makes sense from the plot above, because the probability of outcome=1 should be very low for such a low `pelvic_incidence` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![logistic betas example](../images/logistic_betas_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Changing the $\\beta_0$ value shifts the curve **horizontally**, whereas changing the $\\beta_1$ value changes the **slope** of the curve.\n",
    "\n",
    "The non-bias $\\beta$ coefficients are effectively estimates of how certain you are of the outcome given how much evidence that specific feature gives you. A really high magnitude (positive or negative) value means you are very certain of the outcome, given you know that feature's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure model performance for classification problems?\n",
    "\n",
    "Now that we have a trained model just as we did before with linear regression, what is our **evaluation metric/loss function**?\n",
    "\n",
    "There are two common (inverse) measurements we can make that capture the performance of our classification model:\n",
    "  * **Classification accuracy**: percentage of correct predictions (**reward function**)\n",
    "  * **Classification error**: percentage of incorrect predictions (**loss function**)\n",
    "\n",
    "In our case, we are going to use classification accuracy. Let's compute our classification accuracy after training on the whole dataset, using our just-trained one-feature model and the scikit-learn method `accuracy_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = vertebral_data.outcome_number\n",
    "y_pred = outcome_pred_class\n",
    "print(\"Model accuracy:\",metrics.accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68% is ok, but its not really fantastic. Can we do better? (YES WE CAN!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!!\n",
    "  * Generate the logistic regression model incorporating all of the features we have available to predict `outcome_number` and get the accuracy when training and testing on all data. How much better is this than the case where we trained our model using only `pelvic_incidence`?\n",
    "  * Use train/test split with 70% training, 30% testing and get the test error of the model trained on all features using `train_test_split` like we did during linear regression \n",
    "  * Inspect all of the model coefficients of the model trained on all features. Which feature is the most important for the prediction? Which is the least important?\n",
    "  * What are some problems you can see in using the data like we have been? (Look at the fraction of positive and negative outcomes in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing Logistic Regression with Other Models\n",
    "\n",
    "Logistic regression has some really awesome advantages:\n",
    "\n",
    "  * It is a highly interpretable method (if you remember what the conversions from log-odds to probability are)\n",
    "  * Model training and prediction are fast\n",
    "  * No tuning is required (excluding regularization, which we will talk about later)\n",
    "  * No need to scale features\n",
    "  * Outputs well-calibrated predicted probabilities (the probabilities behave like probabilities)\n",
    "\n",
    "However, logistic regression also has some disadvantages:\n",
    "\n",
    "  * It presumes a linear relationship between the features and the log-odds of the response\n",
    "  * Compared to other, more fancypants modeling approaches, performance is (generally) not competitive with the best supervised learning methods\n",
    "  * Like linear regression for regression, it is sensitive to irrelevant features\n",
    "  * Unless you explicitly code them (we will see how to do that later), logistic regression can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
