{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 7 Agenda:\n",
    "  * **Brief introduction to unsupervised learning**\n",
    "  * **K-means clustering**\n",
    "  * **Evaluating the quality of clustering using Silhouette coefficient**\n",
    "  * **DBScan**\n",
    "  * **Distance metrics and how to not misuse them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             | **Supervised**     | **Unsupervised**      |\n",
    "|-------------|----------------|-------------------|\n",
    "| **Continuous**  | Regression     | **<font color='red'>Clustering</font>**, PCA   |\n",
    "| **Categorical** | Classification | Association Rules |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "In the last class, we talked about the case where we knew what we were trying to predict.\n",
    "\n",
    "We built a model to try to predict the expected gas mileage of a car model, given certain properties of the car (**supervised regression problem**) and a different model to try to predict whether someone had a spinal abnormality given certain measurements of their back and spine (**supervised classification problem**).\n",
    "\n",
    "Both of these were labeled machine learning problems.\n",
    "\n",
    "What if you didn't have any labels for your data but still wanted to see if there was any structure to your dataset? This process is called **unsupervised learning**. We are trying to learn something about our dataset without having access to any outcomes variables (targets, outputs, etc.).\n",
    "\n",
    "We are going to explore 2 common approaches for **unsupervised learning**, called **K-means clustering** and **DBSCAN**.\n",
    "\n",
    "By the end of this lesson you will be able to:\n",
    "  * use k-means and DBSCAN clustering on new data\n",
    "  * explain what the k-parameter in k-means does\n",
    "  * explain what $\\epsilon$ and min_samples parameters in DBSCAN affect\n",
    "  * use euclidean distance for numerical clustering, know what kind of data to use euclidean distance with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we are going to import everything we need for the lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#getting and working with data\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "#visualizing results\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using today is one of the classic datasets used in machine learning, known as the **iris dataset**. This dataset is small and very useful for investigating both clustering and classification problems, but we will be using it exclusively for clustering purposes.\n",
    "\n",
    "You can read more about the dataset [here](https://archive.ics.uci.edu/ml/datasets/Iris). There are 4 features identifying each type of iris:\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "  \n",
    "Going forward, we are going to assume we don't know the kind of iris each sample corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we're getting the data directly off the internet here! how cool is that!\n",
    "iris_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data\",\n",
    "                        names=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\",\"iris_type\"])\n",
    "iris_data_no_names = iris_data.drop(\"iris_type\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_features = iris_data_no_names.columns\n",
    "iris_data_no_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset looks like it has some interesting structure! Let's see if we can't uncover and explore some of it.\n",
    "\n",
    "At first pass, it looks like the dataset has at least 2 distinct clusters, right? So let's use k-means clustering to automatically find 2 clusters in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means clustering** takes a single parameter, which is the number of clusters you want the underlying data to fall into, and attempts to find those clusters automatically as follows:\n",
    "  1. Initially generate random cluster centers equal to the number of clusters\n",
    "  2. For each sample (row), label it with the cluster center it is closest to by computing the [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between it and each cluster center\n",
    "  3. Generate new cluster centers for each cluster based on the labelings for each point.\n",
    "  4. Repeat steps 2-3 until one of the following stopping criteria is met: \n",
    "    * small fraction of samples change labelings\n",
    "    * cluster centers change position by a very small amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "path = \"../images/kmeans/\"\n",
    "files = [ \"kmeans\" + str(x) + \".png\" for x in range(0,13) ]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "for imageName in files:\n",
    "  clear_output()\n",
    "  display(Image(filename=path+imageName))\n",
    "  raw_input()\n",
    "  ax.cla() # turn this off if you'd like to \"build up\" plots\n",
    "\n",
    "plt.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the generated clusters for our dataset, we should understand how **euclidean distance** computes the distance between points in our dataset.\n",
    "\n",
    "Here is the euclidean distance formula for finding the distance between two points, x and y, which have features i=1 to i=k (k features):\n",
    "\n",
    "$d(x,y)=\\sqrt {\\sum_{i=1}^{k}(x_i-y_i)^2}$\n",
    "\n",
    "So, it is simply the length of the path connecting them, as defined using the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem). It is simply finding the square of the difference on each feature, summing across all features, and taking the square root, to make the distance be \"on the same scale\" as the original measurement. (Think about what this means when the feature columns have vastly different scales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of k-means - image compression\n",
    "\n",
    "Take a 1 Megapixel image. Each pixel has 3 color values: R, G, and B, each is a number between 0 and 255 (aka 1 byte). So it takes 3 million bytes to specify the image.\n",
    "\n",
    "We can turn an image into a data set by \"unrolling\" the pixels. Each pixel is a row and the dataset has 3 columns.\n",
    "\n",
    "We could then use k-means to groups the pixels in groups. Then assign each pixel the RGB color of its centroid.\n",
    "\n",
    "What if we used k = 16 groups? It takes half a byte per pixel to store this number. \n",
    "\n",
    "Space savings is 3 bytes/0.5 bytes = 6.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/quant_kmeans_nature.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,random_state=1234)\n",
    "kmeans.fit(iris_data_no_names[iris_data_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names[\"kmeans_2\"] = [\"cluster_\"+str(label) for label in kmeans.labels_]\n",
    "print(iris_data_no_names.dtypes)\n",
    "sns.pairplot(iris_data_no_names,hue=\"kmeans_2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centers found by k-means:\\n\" + \", \".join(iris_data_features) + \"\\n\",\n",
    "      kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_2_cluster_centers = iris_data_no_names.groupby(\"kmeans_2\").mean()\n",
    "print(\"Means of each column :\\n\",iris_2_cluster_centers.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cluster centers on one of the plots we found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data_no_names,x_vars=\"sepal_width\",y_vars=\"sepal_length\",hue=\"kmeans_2\",size=6)\n",
    "plt.scatter(iris_2_cluster_centers.sepal_width, iris_2_cluster_centers.sepal_length, linewidths=3, marker='x', s=200, c='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some of the points labeled strangely relative to what you would expect the overall position of the cluster centers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means** is the first algorithm we will use that **is affected by the scale of every feature**, by virtue of the fact that it uses a single distance metric (euclidean distance) across all features.\n",
    "\n",
    "In practice, this means that for two features with very different scales of values one feature's contribution to the overall distance can dominate all distances found in the other feature:\n",
    "  - Given 2 features and their original scales (difference between maximum and minimum values):\n",
    "    - size in milliimeters\n",
    "    - weight in kilograms\n",
    "  - Distances in peoples sizes (which can vary 2 feet, about ~600mm) will dominate differences in peoples weight (which vary usually no more than ~50kg across people) because the relative scale is ~10x larger for sizes than kilograms.\n",
    "\n",
    "What this means is that **for k-means clustering, features must be scaled to the same ranges of values to contribute \"equally\" to the euclidean distance calculation**.\n",
    "\n",
    "How do we do this in practice?\n",
    "We convert each sample's original value to its z-scored value:\n",
    "\n",
    "$z_i = \\frac{x_i - \\mu}{\\sigma}$\n",
    "\n",
    "So, each row is transformed per-column by:\n",
    "  - subtracting from the element in each row the mean for each feature (column) and then taking this value and\n",
    "  - dividing by that feature's (column's) standard deviation.\n",
    "\n",
    "Z-scoring our data allows us to compare values across columns.\n",
    "Here's the two-line way of generating z-scores for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names.head()\n",
    "iris_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# center and scale the data\n",
    "scaler = StandardScaler()\n",
    "iris_data_scaled = scaler.fit_transform(iris_data_no_names[iris_data_features])\n",
    "iris_data_scaled = pd.DataFrame(iris_data_scaled,columns=iris_data_features)\n",
    "print(\"Before scaling:\\n\", iris_data_no_names[iris_data_features].describe())\n",
    "print(\"\\nAfter scaling:\\n\", np.round(iris_data_scaled.describe(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-means on scaled data\n",
    "km = KMeans(n_clusters=2,random_state=1234)\n",
    "km.fit(iris_data_scaled)\n",
    "iris_data_no_names['kmeans_2_scaled'] = [ \"cluster_\" + str(label) for label in km.labels_ ]\n",
    "print(km.cluster_centers_)\n",
    "iris_data_no_names.groupby('kmeans_2_scaled').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data_no_names,hue=\"kmeans_2_scaled\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference in cluster labels after scaling has been performed? The clusters are much cleaner!\n",
    "\n",
    "What happens if we increase the number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km3 = KMeans(n_clusters=3,random_state=1234)\n",
    "km3.fit(iris_data_scaled)\n",
    "iris_data_no_names['kmeans_3_scaled'] = [ \"cluster_\" + str(label) for label in km3.labels_ ]\n",
    "print(km3.cluster_centers_)\n",
    "iris_data_no_names.groupby('kmeans_3_scaled').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data_no_names,hue=\"kmeans_3_scaled\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!!!\n",
    "  * Generate k-means clustering for 4, 5, and 6 clusters.\n",
    "  * How many samples are there per cluster for each clustering type?\n",
    "  * How do you decide which number of clusters is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using silhouette coefficient to evalualte quality of clustering\n",
    "\n",
    "The [**Silhouette Coefficient**](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) is a common metric for evaluating clustering \"performance\" in situations when the \"true\" cluster assignments are not known.\n",
    "\n",
    "A Silhouette Coefficient is calculated for each observation:\n",
    "\n",
    "$$ SC=\\frac {b−a}{max(a,b)}$$\n",
    "\n",
    "a = mean distance to all other points in its cluster\n",
    "b = mean distance to all other points in the next nearest cluster\n",
    "It ranges from -1 (worst) to 1 (best). A global score is calculated by taking the mean score for all observations.\n",
    "\n",
    "Let's calculate the silhouette coefficient for our original clustering (when k=2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Silhouette score when k=2:\",silhouette_score(iris_data_scaled, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate and plot the silhouette score for between 1 and 15 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(2,16)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km_ss = KMeans(n_clusters=k, random_state=1)\n",
    "    km_ss.fit(iris_data_scaled)\n",
    "    scores.append(silhouette_score(iris_data_scaled, km_ss.labels_))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Coefficient');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!!!\n",
    "\n",
    "I've provided you with the following [seeds dataset](https://archive.ics.uci.edu/ml/datasets/seeds). Each row in the dataset is an individual seed. The individual columns are as follows:\n",
    "  1. seed area A\n",
    "  2. seed perimeter P \n",
    "  3. compactness $C = 4*\\pi*(\\frac {A}{P})^2$\n",
    "  4. length of kernel \n",
    "  5. width of kernel \n",
    "  6. asymmetry coefficient \n",
    "  7. length of kernel groove\n",
    "  \n",
    "In the data I've loaded in, I've explicitly removed the seed labeling so that you can explore the data yourself.\n",
    "\n",
    "Please do the following:\n",
    "  * Perform clustering using a variety of cluster sizes\n",
    "  * Calculate the silhouette score for each cluster size and determine an optimal cluster number\n",
    "  * Visualize the clustering and compute statistics on those clusters. What distinguishes each cluster you've created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_data = pd.read_csv(\"../data/seeds_dataset.txt\",names=[\"area\",\"perimeter\",\"compactness\",\"length\",\"width\",\"asymmetry\",\"kernel_groove_length\",\"seed_type\"])\n",
    "seed_data.drop(\"seed_type\",axis=1,inplace=True)\n",
    "seed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of kmeans:\n",
    "\n",
    "1. Assumes roughly equal-sized (in distance) clusters\n",
    "2. Assumes spherical shape\n",
    "3. Always assigns points to clusters\n",
    "4. Randomness means that best solution isn't guaranteed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kmeans with different-sized clusters](../images/kms1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "clustering_names = ['KMeans']\n",
    "   \n",
    "plt.figure(figsize=(5,10))\n",
    "\n",
    "all_dataset = [noisy_circles, noisy_moons]\n",
    "\n",
    "for i_dataset, dataset in enumerate(all_dataset):\n",
    "    X, y = dataset\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2).fit(X)\n",
    "    \n",
    "    clustering_algorithms = [kmeans]\n",
    "\n",
    "    for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "        # predict cluster memberships\n",
    "        algorithm.fit(X)\n",
    "        y_pred = algorithm.labels_\n",
    "\n",
    "        # plot\n",
    "        plt.subplot(2, 1, i_dataset+1)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred], s=10)\n",
    "        \n",
    "        centers = algorithm.cluster_centers_\n",
    "        center_colors = colors[:len(centers)]\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "        plt.xlim(-2, 2)\n",
    "        plt.ylim(-2, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DBSCAN\n",
    "\n",
    "The second clustering algorithm we are going to investigate is called **DBSCAN** and works a bit differently than K-means.[DBSCAN wiki](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "\n",
    "**DBSCAN** stands for **Density-based spatial clustering of applications with noise**.\n",
    "\n",
    "Whereas K-means does not care about the density of data, **DBSCAN** does, under the assumption that **regions of high density in your data should be treated as clusters**.\n",
    "\n",
    "Furthermore, **DBSCAN does not allow you to specify how many clusters you want.** Instead, you specify 2 parameters:\n",
    "  1. **$\\epsilon$ (epsilon)**: This is the maximum distance between two points to allow them to be neighbors\n",
    "  2. **min_samples**: The number of neighbors a given point is allowed to have to be able to be part of a cluster\n",
    "\n",
    "Any points that don't satisfy the criteria of being close enough to other points are labeled outliers and all fall into a single \"cluster\" (their cluster label by default is -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN works as follows:\n",
    "  1. Choose an arbitrary starting point in your dataset that has not been seen.\n",
    "  2. Retrieve this point's $\\epsilon$-neighborhood (all points that are within a distance **$\\epsilon$** from it), and if it contains at least ***min_samples**, a cluster is started.\n",
    "  3. Otherwise, the point is labeled as an outlier (-1). **Note: This point might later be found in a sufficiently sized ε-environment of a different point and hence be made part of a cluster.**\n",
    "  4. If a point is found to be a dense part of a cluster, its $\\epsilon$-neighborhood is also part of that cluster. All points that are found within the $\\epsilon$-neighborhood are added, as is their own $\\epsilon$-neighborhood when they are also dense.\n",
    "  5. Continue until the density-connected cluster is completely found.\n",
    "  6. Find a new unvisited point to process, rinse and repeat.\n",
    "\n",
    "There are 2 very big differences between **DBSCAN** and **K-means** clustering:\n",
    "  1. **DBSCAN determines the number of clusters automatically whereas K-means requires #of clusters as a parameter.**\n",
    "  2. **DBSCAN takes any distance metric you want, whereas K-means only works with euclidean distance.**\n",
    "  \n",
    "What this means in practice is that **DBSCAN** is very sensitive to:\n",
    "  1. what distance metric you choose\n",
    "  2. what you set $\\epsilon$ and **min_samples** to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through using DBSCAN on the Iris dataset, and see what happens when we use \n",
    "\n",
    "First, we create a DBSCAN object, then we fit it to our scaled iris data. By default, DBSCAN uses euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=1, min_samples=3)\n",
    "db.fit(iris_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names['dbscan_eps1_mpts3'] = [ \"cluster_\" + str(label) for label in db.labels_ ]\n",
    "sns.pairplot(iris_data_no_names,hue=\"dbscan_eps1_mpts3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens as we alter either $\\epsilon$ or min_samples?\n",
    "\n",
    "Let's first alter min_samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db2 = DBSCAN(eps=1, min_samples=10)\n",
    "db2.fit(iris_data_scaled)\n",
    "db2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names['dbscan_eps1_mpts10'] = [ \"cluster_\" + str(label) for label in db2.labels_ ]\n",
    "sns.pairplot(iris_data_no_names,hue=\"dbscan_eps1_mpts10\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we increase **min_samples**, we get more outliers. This makes sense, as increasing the number of points needed to make a cluster becomes harder the larger the number of points we need.\n",
    "\n",
    "Points that are themselves far away from large collections of points now become outliers.\n",
    "\n",
    "What about when we increase $\\epsilon$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db3 = DBSCAN(eps=2, min_samples=3)\n",
    "db3.fit(iris_data_scaled)\n",
    "db3.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names['dbscan_eps2_mpts3'] = [ \"cluster_\" + str(label) for label in db3.labels_ ]\n",
    "sns.pairplot(iris_data_no_names,hue=\"dbscan_eps2_mpts3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, increasing $\\epsilon$ has a different effect from increasing **min_samples**, in that it causes clusters to be merged.\n",
    "\n",
    "This should make sense to you, because as you increase the distance for any points to be $\\epsilon$-connected, it becomes easier to force distant points to be part of the same cluster.\n",
    "\n",
    "Just to hammer this home, lets try to cluster our dataset using a smaller $\\epsilon$, to illustrate that it causes more clusters to form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db4 = DBSCAN(eps=0.6, min_samples=3)\n",
    "db4.fit(iris_data_scaled)\n",
    "db4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_no_names['dbscan_eps0.6_mpts3'] = [ \"cluster_\" + str(label) for label in db4.labels_ ]\n",
    "sns.pairplot(iris_data_no_names,hue=\"dbscan_eps0.6_mpts3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means vs DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "clustering_names = ['KMeans', 'DBSCAN']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "all_datasets = [noisy_circles, noisy_moons]\n",
    "\n",
    "for i_dataset, dataset in enumerate(all_datasets):\n",
    "    X, y = dataset\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2)\n",
    "    dbscan = cluster.DBSCAN(eps=.2)\n",
    "    clustering_algorithms = [kmeans, dbscan]\n",
    "\n",
    "    for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "        # predict cluster memberships\n",
    "        algorithm.fit(X)\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        # plot\n",
    "        plt.subplot(2, len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "        if hasattr(algorithm, 'cluster_centers_'):\n",
    "            centers = algorithm.cluster_centers_\n",
    "            center_colors = colors[:len(centers)]\n",
    "            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "            \n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering comparison code is adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!!!\n",
    "\n",
    "Using the seeds dataset we looked at above, please do the following:\n",
    "  * Perform clustering using a variety of $\\epsilon$ and **min_samples** values\n",
    "  * Calculate the silhouette score for each group of parameters and determine an optimal configuration\n",
    "  * Visualize the clustering and compute statistics on those clusters. What distinguishes each cluster you've created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should I use euclidean distance as a distance metric?\n",
    "\n",
    "Many different kinds of distance metrics exist, but euclidean distance is the most commonly used (and misused) distance metric.\n",
    "\n",
    "It is useful when:\n",
    "* All of the features (columns) in your data are numeric\n",
    "* There are not many feature dimensions (<100)\n",
    "\n",
    "It is not useful when:\n",
    "* Columns in your data are composed of either strings or categories, or your columns have both numeric and categorical features.\n",
    "* You have 1000s of features in your dataset (because of the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))\n",
    "\n",
    "We will talk about other distance metrics and when they should be used next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "**K-means**:\n",
    "  * is a good off-the-shelf clustering algorithm when youre dealing with numerical data and have some idea of the number of clusters you are looking for in your dataset\n",
    "  * requires feature scaling\n",
    "  * requires specifiying the number of clusters\n",
    "  * can only use the euclidean distance as a distance metric\n",
    "  * is non-deterministic (will generate different cluster labelings depending on initial conditions)\n",
    "\n",
    "**DBSCAN**:\n",
    "  * is a good clustering algorithm when you are attempting to find dense and non-dense regions in your feature space but dont know the number of clusters you're looking for.\n",
    "  * requires feature scaling\n",
    "  * requires specifying a distance metric (euclidean is default)\n",
    "  * requires specifying a minimum distance between points, $\\epsilon$\n",
    "  * requires specifying a minimum number of points to be called a \"cluster\" (min_samples), minimum is 2\n",
    "  * is non-deterministic (when points are on the border between two clusters, can be assigned to either one, depending on order in which un-visited points are visited).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
